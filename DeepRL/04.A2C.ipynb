{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce & Actor-Advantage Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[You can find the original paper here](https://arxiv.org/pdf/1602.01783.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "In this tutorial we will focus on Deep Reinforcement Learning with **Reinforce** and the **Actor-Advantage Critic** algorithm. This tutorial is composed of:\n",
    "* A quick reminder of the RL setting,\n",
    "* A theoritical approch of Reinforce\n",
    "* A theoritical approch of A2C,\n",
    "* An introduction to the deep learning framework: **PyTorch**, \n",
    "* A coding part with experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you already know PyTorch you can skip this part. From this part on we assume that you have some experience with Python and Numpy.*\n",
    "\n",
    "PyTorch is a Python package that provides two high-level features:\n",
    "- Tensor computation (like NumPy) with strong GPU acceleration\n",
    "- Deep neural networks built on a tape-based autograd system\n",
    "\n",
    "At a granular level, PyTorch is a library that consists of the following components:\n",
    "\n",
    "| Component | Description |\n",
    "| ---- | --- |\n",
    "| [**torch**](https://pytorch.org/docs/stable/torch.html) | a Tensor library like NumPy, with strong GPU support |\n",
    "| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n",
    "| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n",
    "| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | a neural networks library deeply integrated with autograd designed for maximum flexibility |\n",
    "| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n",
    "| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch works in a very similar way as Numpy and PyTorch's Tensors are the equivalent of Numpy's Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize an zero filled tensor just like in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert an array to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can transform a tensor to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sum, substract, multiply arrays just like in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 4, 3],\n",
      "        [7, 3, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0,10,(2,3))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 9, 1],\n",
      "        [1, 4, 0]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.randint(0,10,(2,3))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = tensor([[11, 13,  4],\n",
      "        [ 8,  7,  1]])\n",
      "a * b = tensor([[28, 36,  3],\n",
      "        [ 7, 12,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(f'a + b = {a + b}')\n",
    "print(f'a * b = {a * b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make matrix products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[67, 23],\n",
       "        [56, 19]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOGRAD: automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "``torch.Tensor`` is the central class of the package. If you set its attribute\n",
    "``.requires_grad`` as ``True``, it starts to track all operations on it. When\n",
    "you finish your computation you can call ``.backward()`` and have all the\n",
    "gradients computed automatically. The gradient for this tensor will be\n",
    "accumulated into ``.grad`` attribute.\n",
    "\n",
    "To stop a tensor from tracking history, you can call ``.detach()`` to detach\n",
    "it from the computation history, and to prevent future computation from being\n",
    "tracked.\n",
    "\n",
    "To prevent tracking history (and using memory), you can also wrap the code block\n",
    "in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n",
    "model because the model may have trainable parameters with\n",
    "``requires_grad=True``, but for which we don't need the gradients.\n",
    "\n",
    "There’s one more class which is very important for autograd\n",
    "implementation - a ``Function``.\n",
    "\n",
    "``Tensor`` and ``Function`` are interconnected and build up an acyclic\n",
    "graph, that encodes a complete history of computation. Each tensor has\n",
    "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Tensor`` (except for Tensors created by the user - their\n",
    "``grad_fn is None``).\n",
    "\n",
    "If you want to compute the derivatives, you can call ``.backward()`` on\n",
    "a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element\n",
    "data), you don’t need to specify any arguments to ``backward()``,\n",
    "however if it has more elements, you need to specify a ``gradient``\n",
    "argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder of the RL setting\n",
    "\n",
    "As always we will consider a MDP $M = (\\mathcal{X}, \\mathcal{A}, p, r, \\gamma)$ with:\n",
    "* $\\mathcal{X}$ the state space,\n",
    "* $\\mathcal{A}$ the action space,\n",
    "* $p(x^\\prime \\mid x, a)$ the transition probability,\n",
    "* $r(x, a, x^\\prime)$ the reward of the transition $(x, a, x^\\prime)$,\n",
    "* $\\gamma \\in [0,1)$ is the discount factor.\n",
    "\n",
    "A policy $\\pi$ is a mapping from the state space $\\mathcal{X}$ to the probability of selecting each action.\n",
    "\n",
    "The action value function of a policy is the overall expected reward from a state action. $Q^\\pi(s, a) = \\mathbb{E}_{\\tau \\sim \\pi}\\big[R(\\tau) \\mid s_0=s, a_0=a\\big]$ where $R(\\tau)$ is the random variable defined as the sum of the discounted reward.\n",
    "\n",
    "The goal is to maximize the agent's reward.\n",
    "\n",
    "$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\big[ \\sum_{t} \\gamma^t R_t \\mid x_0, \\pi \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym + Random agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Reinforce is an actor-based **on policy** method. The policy $\\pi_{\\theta}$ is parametrized by a function approximator (e.g. a neural network).\n",
    "\n",
    "Recall: $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\big[ \\sum_{t} \\gamma^t R_t \\mid x_0, \\pi \\big].$$\n",
    "\n",
    "To update the parameters $\\theta$ of the policy, one has to do gradient ascent: $\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_{\\theta}J(\\pi_{\\theta})|_{\\theta_{k}}$.\n",
    "\n",
    "Advantages of this approach:\n",
    "- Compared to a Q-learning approach, here the policy is directly parametrized so a small change of the parameters will not dramatically change the policy whereas this is not the case for Q-learning approaches.\n",
    "- The stochasticity of the policy allows exploration. In off policy learning, one has to deal with both a behaviour policy and an exploration policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Theorem\n",
    "\n",
    "Q.1: Prove the Policy Gradient Theorem: $$ \\displaystyle \\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}\\left[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau)}\\right]$$\n",
    "\n",
    "Hint 1 - The probability of a trajectory $\\tau = (s_{0}, a_{0},\\dots, s_{T+1}$) with action chosen from $\\displaystyle \\pi_{\\theta}$ is $P(\\tau|\\theta) = \\rho_{0}(s_{0})\\prod_{t=0}^{T}P\\left(s_{t+1}|s_{t}, a_{t}\\right) \\pi_{\\theta}(a_{t}|s_{t})$\n",
    "\n",
    "Hint 2 - Gradient-log trick: $  \\nabla_{\\theta}P(\\tau|\\theta)= P(\\tau|\\theta)\\nabla_{\\theta}\\log P(\\tau|\\theta). $\n",
    "\n",
    "The policy gradient can therefore be approximated with:\n",
    "$$ \\hat{g} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.2: Implement the REINFORCE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code of reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dim_observation, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.dim_observation = dim_observation\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=self.dim_observation, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=self.n_actions),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action = torch.multinomial(self.forward(state), 1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, dim_observation, n_actions, gamma):\n",
    "        self.model = Model(dim_observation=dim_observation, n_actions=n_actions)\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = torch.optim.Adam(self.model.net.parameters(), lr=0.01)\n",
    "    \n",
    "    def _make_returns(self, rewards):\n",
    "        returns = np.zeros_like(rewards)\n",
    "        returns[-1] = rewards[-1]\n",
    "        for t in reversed(range(len(rewards) - 1)):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1]\n",
    "        return returns\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, env, n_trajectories, n_update):\n",
    "        for episode in range(n_update):\n",
    "            mean_reward, std_reward, min_reward, max_reward = self.optimize_model(env, n_trajectories)\n",
    "            print(\"Episode {}\".format(episode+1))\n",
    "            print(\"Reward:μσmM {:.2f} {:.2f} {:.2f} {:.2f}\"\n",
    "              .format(mean_reward, std_reward, min_reward, max_reward))\n",
    "\n",
    "    def evaluate(self, env, n_trajectories):\n",
    "        reward_trajectories = np.zeros(n_trajectories)\n",
    "        for i in range(n_trajectories):\n",
    "            # New episode\n",
    "            observation = env.reset()\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            reward_episode = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                env.render()\n",
    "                action = self.model.select_action(observation)\n",
    "                observation, reward, done, info = env.step(int(action))\n",
    "                observation = torch.tensor(observation, dtype=torch.float)\n",
    "                reward_episode += reward\n",
    "            \n",
    "            reward_trajectories[i] = reward_episode\n",
    "        env.close()\n",
    "        print(\"Reward:μσmM {:.2f} {:.2f} {:.2f} {:.2f}\"\n",
    "              .format(reward_trajectories.mean(), reward_trajectories.std(), \n",
    "                      reward_trajectories.min(), reward_trajectories.max()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent(BaseAgent):\n",
    "    def __init__(self, dim_observation, n_actions, gamma):\n",
    "        super(SimpleAgent, self).__init__(dim_observation=dim_observation, n_actions=n_actions, gamma=gamma)\n",
    "    \n",
    "    def optimize_model(self, env, n_trajectories):\n",
    "        weighted_logproba = torch.zeros(n_trajectories)\n",
    "        reward_trajectories = np.zeros(n_trajectories)\n",
    "\n",
    "        for i in range(n_trajectories):\n",
    "            # New episode\n",
    "            observation = env.reset()\n",
    "            rewards_episode = []\n",
    "            logproba_episode = []\n",
    "            discount_factor = 1\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.model.select_action(observation)\n",
    "                logproba_episode.append(torch.log(self.model.forward(observation))[action])\n",
    "                # Interaction with the environment\n",
    "                observation, reward, done, info = env.step(int(action))\n",
    "                observation = torch.tensor(observation, dtype=torch.float)\n",
    "                rewards_episode.append(discount_factor * reward)\n",
    "                discount_factor *= self.gamma\n",
    "            \n",
    "            cum_rewards_episode = np.sum(rewards_episode)\n",
    "            weighted_logproba[i]= cum_rewards_episode * torch.cat(logproba_episode).sum()\n",
    "            reward_trajectories[i] = cum_rewards_episode\n",
    "            \n",
    "        loss = - weighted_logproba.mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        # Compute the gradient \n",
    "        loss.backward()\n",
    "        # Do the gradient descent step\n",
    "        self.optimizer.step()\n",
    "        return reward_trajectories.mean(), reward_trajectories.std(), reward_trajectories.min(), reward_trajectories.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't let the past distract you\n",
    "\n",
    "- The sum of rewards during one episode has a high variance which affects the performance of this version of REINFORCE.\n",
    "- To assess the quality of an action, it make more sens to take into consideration only the rewards obtained after taking this action.\n",
    "- It can be proven that $$  \\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}\\left[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) \\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})}\\right].$$\n",
    "- Bonus: proof of this claim.\n",
    "- This has for effect to reduce the variance. Past rewards have zero mean but nonzero variance so they just add noise.  \n",
    "\n",
    "Q3: Implement this enhance version of REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnhanceAgent(BaseAgent):\n",
    "    def __init__(self, dim_observation, n_actions, gamma):\n",
    "        super(EnhanceAgent, self).__init__(dim_observation=dim_observation, n_actions=n_actions, gamma=gamma)\n",
    "    \n",
    "    def optimize_model(self, env, n_trajectories):\n",
    "        weighted_logproba = torch.zeros(n_trajectories)\n",
    "        reward_trajectories = np.zeros(n_trajectories)\n",
    "\n",
    "        for i in range(n_trajectories):\n",
    "            # New episode\n",
    "            observation = env.reset()\n",
    "            rewards_episode = []\n",
    "            logproba_episode = []\n",
    "            discount_factor = 1\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.model.select_action(observation)\n",
    "                logproba_episode.append(torch.log(self.model.forward(observation))[action])\n",
    "                # Interaction with the environment\n",
    "                observation, reward, done, info = env.step(int(action))\n",
    "                observation = torch.tensor(observation, dtype=torch.float)\n",
    "                rewards_episode.append(discount_factor * reward)\n",
    "                discount_factor *= self.gamma\n",
    "            \n",
    "            inverse_cum_rewards = self._make_returns(rewards_episode)\n",
    "            reward_trajectories[i] = inverse_cum_rewards[0]\n",
    "            inverse_cum_rewards = torch.tensor(inverse_cum_rewards, dtype=torch.float)\n",
    "            weighted_logproba[i]= torch.sum(inverse_cum_rewards * torch.cat(logproba_episode))\n",
    "            \n",
    "        loss = - weighted_logproba.mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        # Compute the gradient \n",
    "        loss.backward()\n",
    "        # Do the gradient descent step\n",
    "        self.optimizer.step()\n",
    "        return reward_trajectories.mean(), reward_trajectories.std(), reward_trajectories.min(), reward_trajectories.max()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Reward:μσmM 19.44 9.94 8.00 48.00\n",
      "Episode 2\n",
      "Reward:μσmM 21.26 10.15 8.00 58.00\n",
      "Episode 3\n",
      "Reward:μσmM 20.42 9.13 8.00 56.00\n",
      "Episode 4\n",
      "Reward:μσmM 19.28 8.99 8.00 55.00\n",
      "Episode 5\n",
      "Reward:μσmM 20.62 7.93 10.00 37.00\n",
      "Episode 6\n",
      "Reward:μσmM 22.54 12.58 8.00 77.00\n",
      "Episode 7\n",
      "Reward:μσmM 21.64 10.05 9.00 54.00\n",
      "Episode 8\n",
      "Reward:μσmM 26.70 13.61 11.00 89.00\n",
      "Episode 9\n",
      "Reward:μσmM 24.00 13.84 10.00 64.00\n",
      "Episode 10\n",
      "Reward:μσmM 22.54 12.84 9.00 64.00\n",
      "Episode 11\n",
      "Reward:μσmM 23.64 12.61 9.00 89.00\n",
      "Episode 12\n",
      "Reward:μσmM 25.46 12.30 10.00 69.00\n",
      "Episode 13\n",
      "Reward:μσmM 23.20 8.82 10.00 49.00\n",
      "Episode 14\n",
      "Reward:μσmM 23.56 11.03 9.00 49.00\n",
      "Episode 15\n",
      "Reward:μσmM 23.94 18.52 10.00 132.00\n",
      "Episode 16\n",
      "Reward:μσmM 31.60 20.62 10.00 93.00\n",
      "Episode 17\n",
      "Reward:μσmM 29.34 18.09 10.00 93.00\n",
      "Episode 18\n",
      "Reward:μσmM 29.42 19.27 9.00 112.00\n",
      "Episode 19\n",
      "Reward:μσmM 31.10 16.00 12.00 88.00\n",
      "Episode 20\n",
      "Reward:μσmM 29.92 15.94 9.00 77.00\n",
      "Episode 21\n",
      "Reward:μσmM 28.12 13.73 11.00 69.00\n",
      "Episode 22\n",
      "Reward:μσmM 35.40 24.27 13.00 150.00\n",
      "Episode 23\n",
      "Reward:μσmM 32.56 14.84 16.00 79.00\n",
      "Episode 24\n",
      "Reward:μσmM 37.06 21.17 11.00 79.00\n",
      "Episode 25\n",
      "Reward:μσmM 37.08 19.48 11.00 117.00\n",
      "Episode 26\n",
      "Reward:μσmM 44.38 24.68 11.00 117.00\n",
      "Episode 27\n",
      "Reward:μσmM 46.08 31.70 12.00 137.00\n",
      "Episode 28\n",
      "Reward:μσmM 44.20 26.65 10.00 155.00\n",
      "Episode 29\n",
      "Reward:μσmM 47.00 28.51 12.00 140.00\n",
      "Episode 30\n",
      "Reward:μσmM 53.60 30.39 15.00 168.00\n",
      "Episode 31\n",
      "Reward:μσmM 56.32 32.54 17.00 136.00\n",
      "Episode 32\n",
      "Reward:μσmM 60.50 38.59 14.00 196.00\n",
      "Episode 33\n",
      "Reward:μσmM 67.68 41.87 13.00 197.00\n",
      "Episode 34\n",
      "Reward:μσmM 68.28 38.77 10.00 166.00\n",
      "Episode 35\n",
      "Reward:μσmM 76.04 41.92 19.00 207.00\n",
      "Episode 36\n",
      "Reward:μσmM 79.12 38.10 18.00 168.00\n",
      "Episode 37\n",
      "Reward:μσmM 86.44 51.27 12.00 253.00\n",
      "Episode 38\n",
      "Reward:μσmM 80.52 49.29 16.00 263.00\n",
      "Episode 39\n",
      "Reward:μσmM 105.08 55.20 21.00 284.00\n",
      "Episode 40\n",
      "Reward:μσmM 107.86 42.04 33.00 225.00\n",
      "Episode 41\n",
      "Reward:μσmM 117.78 61.54 15.00 279.00\n",
      "Episode 42\n",
      "Reward:μσmM 122.62 50.99 15.00 270.00\n",
      "Episode 43\n",
      "Reward:μσmM 145.30 49.60 38.00 286.00\n",
      "Episode 44\n",
      "Reward:μσmM 136.22 51.79 36.00 268.00\n",
      "Episode 45\n",
      "Reward:μσmM 138.76 65.77 35.00 389.00\n",
      "Episode 46\n",
      "Reward:μσmM 137.96 48.91 33.00 388.00\n",
      "Episode 47\n",
      "Reward:μσmM 151.18 40.50 98.00 283.00\n",
      "Episode 48\n",
      "Reward:μσmM 162.58 50.90 17.00 290.00\n",
      "Episode 49\n",
      "Reward:μσmM 207.94 75.27 72.00 411.00\n",
      "Episode 50\n",
      "Reward:μσmM 210.12 97.54 41.00 495.00\n"
     ]
    }
   ],
   "source": [
    "# Make the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Seeds\n",
    "seed = 1\n",
    "env.seed(seed=seed)\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed=seed)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "dim_observation = observation.shape[0]\n",
    "\n",
    "Agent = SimpleAgent(dim_observation=dim_observation, n_actions=n_actions, gamma=1)\n",
    "Agent.train(env=env, n_trajectories=50, n_update=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Reward:μσmM 16.20 8.98 8.00 61.00\n",
      "Episode 2\n",
      "Reward:μσmM 17.10 8.74 10.00 48.00\n",
      "Episode 3\n",
      "Reward:μσmM 19.70 12.42 9.00 80.00\n",
      "Episode 4\n",
      "Reward:μσmM 20.12 8.60 10.00 39.00\n",
      "Episode 5\n",
      "Reward:μσmM 18.44 7.33 10.00 45.00\n",
      "Episode 6\n",
      "Reward:μσmM 20.18 9.06 9.00 52.00\n",
      "Episode 7\n",
      "Reward:μσmM 18.46 8.05 9.00 53.00\n",
      "Episode 8\n",
      "Reward:μσmM 23.22 11.99 9.00 64.00\n",
      "Episode 9\n",
      "Reward:μσmM 22.10 9.45 11.00 57.00\n",
      "Episode 10\n",
      "Reward:μσmM 24.42 12.38 10.00 82.00\n",
      "Episode 11\n",
      "Reward:μσmM 24.46 14.12 10.00 77.00\n",
      "Episode 12\n",
      "Reward:μσmM 23.92 10.57 9.00 51.00\n",
      "Episode 13\n",
      "Reward:μσmM 28.94 16.70 9.00 86.00\n",
      "Episode 14\n",
      "Reward:μσmM 27.22 12.61 10.00 68.00\n",
      "Episode 15\n",
      "Reward:μσmM 28.68 20.64 11.00 118.00\n",
      "Episode 16\n",
      "Reward:μσmM 35.28 18.55 10.00 93.00\n",
      "Episode 17\n",
      "Reward:μσmM 29.28 17.24 11.00 83.00\n",
      "Episode 18\n",
      "Reward:μσmM 28.38 16.87 10.00 92.00\n",
      "Episode 19\n",
      "Reward:μσmM 35.26 24.87 9.00 120.00\n",
      "Episode 20\n",
      "Reward:μσmM 34.96 21.66 9.00 123.00\n",
      "Episode 21\n",
      "Reward:μσmM 40.34 23.07 12.00 100.00\n",
      "Episode 22\n",
      "Reward:μσmM 33.68 22.48 9.00 158.00\n",
      "Episode 23\n",
      "Reward:μσmM 36.62 26.67 9.00 164.00\n",
      "Episode 24\n",
      "Reward:μσmM 41.04 21.73 10.00 106.00\n",
      "Episode 25\n",
      "Reward:μσmM 40.30 22.47 10.00 118.00\n",
      "Episode 26\n",
      "Reward:μσmM 41.76 23.30 12.00 117.00\n",
      "Episode 27\n",
      "Reward:μσmM 49.74 26.92 11.00 139.00\n",
      "Episode 28\n",
      "Reward:μσmM 48.06 36.86 8.00 195.00\n",
      "Episode 29\n",
      "Reward:μσmM 56.52 35.21 11.00 163.00\n",
      "Episode 30\n",
      "Reward:μσmM 57.38 30.98 14.00 148.00\n",
      "Episode 31\n",
      "Reward:μσmM 57.18 28.60 15.00 151.00\n",
      "Episode 32\n",
      "Reward:μσmM 70.94 43.39 13.00 207.00\n",
      "Episode 33\n",
      "Reward:μσmM 73.22 43.95 12.00 190.00\n",
      "Episode 34\n",
      "Reward:μσmM 71.10 42.39 14.00 213.00\n",
      "Episode 35\n",
      "Reward:μσmM 86.04 50.77 12.00 231.00\n",
      "Episode 36\n",
      "Reward:μσmM 87.96 49.33 12.00 224.00\n",
      "Episode 37\n",
      "Reward:μσmM 105.20 54.65 23.00 240.00\n",
      "Episode 38\n",
      "Reward:μσmM 100.76 53.18 15.00 286.00\n",
      "Episode 39\n",
      "Reward:μσmM 117.04 60.89 29.00 299.00\n",
      "Episode 40\n",
      "Reward:μσmM 122.16 69.39 35.00 339.00\n",
      "Episode 41\n",
      "Reward:μσmM 134.70 64.43 27.00 299.00\n",
      "Episode 42\n",
      "Reward:μσmM 128.86 77.36 12.00 488.00\n",
      "Episode 43\n",
      "Reward:μσmM 150.58 86.45 17.00 350.00\n",
      "Episode 44\n",
      "Reward:μσmM 211.46 92.16 76.00 406.00\n",
      "Episode 45\n",
      "Reward:μσmM 217.12 110.08 34.00 500.00\n",
      "Episode 46\n",
      "Reward:μσmM 253.92 133.09 60.00 500.00\n",
      "Episode 47\n",
      "Reward:μσmM 287.70 146.66 61.00 500.00\n",
      "Episode 48\n",
      "Reward:μσmM 304.28 128.39 75.00 500.00\n",
      "Episode 49\n",
      "Reward:μσmM 347.94 129.43 47.00 500.00\n",
      "Episode 50\n",
      "Reward:μσmM 405.14 116.25 128.00 500.00\n"
     ]
    }
   ],
   "source": [
    "Agent = EnhanceAgent(dim_observation=dim_observation, n_actions=n_actions, gamma=1)\n",
    "Agent.train(env=env, n_trajectories=50, n_update=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From REINFORCE to A2C\n",
    "\n",
    "### The idea behind A2C\n",
    "\n",
    "The need of a critic.\n",
    "\n",
    "### A2C"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
