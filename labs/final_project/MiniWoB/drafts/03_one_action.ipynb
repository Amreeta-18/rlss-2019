{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've found during the long \"Troubleshooting session\", asynchronous observations turn even simple environment into POMDP domain: from one single image we cannot make predictions about the future trajectory, which builds a fundamental obstacle for value-iteration method.\n",
    "\n",
    "Funny enough: PG-based methods are able to converge in the same situation. As they don't need to predict Q-values to get the policy, their task is much simpler: they just need to learn where to click, without bothering much about  the future reward. So, they just click and don't think much.\n",
    "\n",
    "Can we use the same approach to make DQN work in MiniWoB? Yes, why not, there are two ways of doing this:\n",
    "1. turn the environment into bandit-style -- agent is allowed to click once and need to wait for the outcome. It won't be very efficient, but it will work. \n",
    "2. stop pretenting our env is MDP and deal with its POMDP nature. Nomal way will be to extend the observations with some kind of \"state\" or \"history\" to make it MDP again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit approach to ClickDialog\n",
    "\n",
    "Let's explore the first path, but minimize the modifications: implement the environment wrapper which will allow agent to execute one single action and then wait for the final reward (which will happen in 10 seconds in any case due to MiniWoB timeout). \n",
    "\n",
    "Then we'll be able to plug this wrapper into the code we already have written, hopefully, this will make our DQN to converge.\n",
    "\n",
    "Of course, this is an overkill -- you're more than welcome to implement normal bandit methods and compare them to this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56916832e628\r\n"
     ]
    }
   ],
   "source": [
    "!./containers_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting container 1\n",
      "e671bd7f02069d9714c29eebc6dab685c9ed6151a9a9fec8b9f5fb03f5d4fe00\n"
     ]
    }
   ],
   "source": [
    "!./containers_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKER_IMAGE = \"shmuma/miniwob:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/runtimes/__init__.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  spec = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import time\n",
    "import numpy as np\n",
    "import universe\n",
    "from typing import List, Optional, Tuple\n",
    "from universe import vectorized\n",
    "from universe.wrappers.experimental import SoftmaxClickMouse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build connection endpoints for set of containers\n",
    "def remotes_url(port_ofs=0, hostname='localhost', count=8):\n",
    "    hosts = [\"%s:%d+%d\" % (hostname, 5900 + ofs, 15900 + ofs) for ofs in range(port_ofs, port_ofs+count)]\n",
    "    return \"vnc://\" + \",\".join(hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniWoBCropper(vectorized.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Crops the WoB area and converts the observation into PyTorch (C, H, W) format.\n",
    "    \"\"\"\n",
    "    # Area of interest\n",
    "    WIDTH = 160\n",
    "    HEIGHT = 210\n",
    "    X_OFS = 10\n",
    "    Y_OFS = 75\n",
    "    \n",
    "    def __init__(self, env, keep_text=False):\n",
    "        super(MiniWoBCropper, self).__init__(env)\n",
    "        self.keep_text = keep_text\n",
    "        img_space = gym.spaces.Box(low=0, high=255, shape=(3, self.HEIGHT, self.WIDTH))\n",
    "        if keep_text:\n",
    "            self.observation_space = gym.spaces.Tuple(spaces=(img_space, gym.spaces.Space))\n",
    "        else:\n",
    "            self.observation_space = img_space\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        res = []\n",
    "        for obs in observation_n:\n",
    "            if obs is None:\n",
    "                res.append(obs)\n",
    "                continue\n",
    "            img = obs['vision'][self.Y_OFS:self.Y_OFS+self.HEIGHT, self.X_OFS:self.X_OFS+self.WIDTH, :]\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "            if self.keep_text:\n",
    "                text = \" \".join(map(lambda d: d.get('instruction', ''), obs.get('text', [{}])))\n",
    "                res.append((img, text))\n",
    "            else:\n",
    "                res.append(img)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, 5, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast], net: nn.Module, gamma: float, device=\"cpu\"):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    last_states_v = torch.tensor(last_states).to(device)\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v + rewards_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Wrapper class (practice)\n",
    "\n",
    "Below you need to implement the wrapper. Do not forget that everything is vectorized! Don't need to implement it in a generic way -- case for single environment would be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SingleShotWrapper(vectorized.Wrapper):\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action_n):\n",
    "        raise NotImplementedError\n",
    "        return obs_n, reward_n, [True], {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-01 21:22:46,195] Making new env: wob.mini.ClickDialog-v0\n",
      "[2019-07-01 21:22:46,196] Using SoftmaxClickMouse with action_region=(10, 125, 170, 285), noclick_regions=[]\n",
      "[2019-07-01 21:22:46,197] SoftmaxClickMouse noclick regions removed 0 of 256 actions\n",
      "[2019-07-01 21:22:46,200] Using the golang VNC implementation\n",
      "[2019-07-01 21:22:46,201] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-07-01 21:22:46,271] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-01 21:22:51,442] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: [0.0], [False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-01 21:22:51,516] [0:localhost:5900] Initial reset complete: episode_id=326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-01 21:23:02,615] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-07-01 21:23:02,678] [0:localhost:5900] Initial reset complete: episode_id=328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [-1.0], [True]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wob.mini.ClickDialog-v0\")\n",
    "\n",
    "env = SoftmaxClickMouse(env)\n",
    "env = MiniWoBCropper(env)\n",
    "env = SingleShotWrapper(env)\n",
    "url = remotes_url(count=1)\n",
    "\n",
    "# Note FPS=10\n",
    "env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=1, vnc_kwargs={\n",
    "        'encoding': 'tight', 'compress_level': 0,\n",
    "        'fine_quality_level': 100, 'subsample_level': 0\n",
    "    })\n",
    "obs = env.reset()\n",
    "\n",
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-01 21:23:11,374] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "Init: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [0.0], [False]\n",
      "Loop: [-1.0], [True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[[181, 182, 182, ..., 182, 182, 182],\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [254, 255, 255, ..., 255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [255, 255, 255, ..., 255, 255, 255]],\n",
       "  \n",
       "         [[179, 180, 180, ..., 180, 180, 180],\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [255, 255, 255, ..., 255, 255, 255]],\n",
       "  \n",
       "         [[182, 183, 183, ..., 183, 183, 183],\n",
       "          [  0,   0,   0, ...,   0,   0,   0],\n",
       "          [  0,   0,   0, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [255, 255, 255, ..., 255, 255, 255],\n",
       "          [255, 255, 255, ..., 255, 255, 255]]], dtype=uint8)],\n",
       " [-1.0],\n",
       " [True],\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = env.reset()\n",
    "print(o)\n",
    "env.step([5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper class (solution)\n",
    "\n",
    "**Spoiler alert!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleShotWrapper(vectorized.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(SingleShotWrapper, self).__init__(env)\n",
    "        self.fresh = True\n",
    "        \n",
    "    def reset(self):\n",
    "        self.fresh = True\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.fresh:\n",
    "            self.fresh = False\n",
    "            return self.env.step(action)  \n",
    "        obs_n, reward_n, is_done_n, _ = self.env.step(action)\n",
    "        while not is_done_n[0]:            \n",
    "            o_n, r_n, is_done_n, _ = self.env.step(action)\n",
    "            reward_n[0] += r_n[0]\n",
    "        self.fresh = True\n",
    "        return obs_n, reward_n, [True], {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params are tweaked to reflect our one episode -> one sample rate\n",
    "GAMMA = 1.0              # gamma is totally non-relevant in one-step case\n",
    "REPLAY_SIZE = 1000\n",
    "MIN_REPLAY = 20\n",
    "TGT_SYNC = 10\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "DEVICE = \"cuda\"  \n",
    "#DEVICE = \"cpu\"\n",
    "\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.2\n",
    "STEPS_EPSILON = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-02 12:04:54,650] Making new env: wob.mini.ClickDialog-v0\n",
      "[2019-07-02 12:04:54,653] Using SoftmaxClickMouse with action_region=(10, 125, 170, 285), noclick_regions=[]\n",
      "[2019-07-02 12:04:54,654] SoftmaxClickMouse noclick regions removed 0 of 256 actions\n",
      "[2019-07-02 12:04:54,656] Using the golang VNC implementation\n",
      "[2019-07-02 12:04:54,656] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-07-02 12:04:54,681] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n",
      "[2019-07-02 12:05:11,106] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-07-02 12:05:11,119] [0:localhost:5900] Initial reset complete: episode_id=45\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wob.mini.ClickDialog-v0\")\n",
    "\n",
    "env = SoftmaxClickMouse(env)\n",
    "env = MiniWoBCropper(env)\n",
    "env = SingleShotWrapper(env)\n",
    "url = remotes_url(count=1)\n",
    "\n",
    "# Note FPS=10\n",
    "env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=1, vnc_kwargs={\n",
    "        'encoding': 'tight', 'compress_level': 0,\n",
    "        'fine_quality_level': 100, 'subsample_level': 0\n",
    "    })\n",
    "obs = env.reset()\n",
    "\n",
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "parent_selector = ptan.actions.ArgmaxActionSelector()\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=INITIAL_EPSILON, selector=parent_selector) \n",
    "agent = ptan.agent.DQNAgent(net, action_selector, device=DEVICE)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, vectorized=True)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "optimizer = optim.Adam(net.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-02 12:05:16,784] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Done 1 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.11 steps/s, eps=1.00\n",
      "3: Done 2 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "4: Done 3 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "5: Done 4 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "6: Done 5 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "7: Done 6 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "8: Done 7 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "9: Done 8 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "10: Done 9 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "11: Done 10 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "12: Done 11 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "13: Done 12 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "14: Done 13 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "15: Done 14 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "16: Done 15 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "17: Done 16 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "18: Done 17 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "19: Done 18 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "20: Done 19 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=1.00\n",
      "20: nets synced, mean loss for last 10 steps = 1.015\n",
      "21: Done 20 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=1.00\n",
      "22: Done 21 episodes, last 10 means: reward=-0.800, steps=2.000, speed=0.10 steps/s, eps=0.95\n",
      "23: Done 22 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.90\n",
      "24: Done 23 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.85\n",
      "25: Done 24 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.80\n",
      "26: Done 25 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.75\n",
      "27: Done 26 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.70\n",
      "28: Done 27 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.65\n",
      "29: Done 28 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.60\n",
      "30: Done 29 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.55\n",
      "30: nets synced, mean loss for last 10 steps = 0.915\n",
      "31: Done 30 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.50\n",
      "32: Done 31 episodes, last 10 means: reward=-1.100, steps=2.000, speed=0.10 steps/s, eps=0.45\n",
      "33: Done 32 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.40\n",
      "34: Done 33 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.35\n",
      "35: Done 34 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.30\n",
      "36: Done 35 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.25\n",
      "37: Done 36 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.50 steps/s, eps=0.20\n",
      "38: Done 37 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "39: Done 38 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "40: Done 39 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "40: nets synced, mean loss for last 10 steps = 0.811\n",
      "41: Done 40 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "42: Done 41 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "43: Done 42 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "44: Done 43 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "45: Done 44 episodes, last 10 means: reward=-0.711, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "46: Done 45 episodes, last 10 means: reward=-0.811, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "47: Done 46 episodes, last 10 means: reward=-0.811, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "48: Done 47 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "49: Done 48 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "50: Done 49 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "50: nets synced, mean loss for last 10 steps = 0.849\n",
      "51: Done 50 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "52: Done 51 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "53: Done 52 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "54: Done 53 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "55: Done 54 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "56: Done 55 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "57: Done 56 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "58: Done 57 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "59: Done 58 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "60: Done 59 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "60: nets synced, mean loss for last 10 steps = 0.910\n",
      "61: Done 60 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "62: Done 61 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "63: Done 62 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "64: Done 63 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "65: Done 64 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "66: Done 65 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "67: Done 66 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "68: Done 67 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "69: Done 68 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "70: Done 69 episodes, last 10 means: reward=-0.800, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "70: nets synced, mean loss for last 10 steps = 0.799\n",
      "71: Done 70 episodes, last 10 means: reward=-0.800, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "72: Done 71 episodes, last 10 means: reward=-0.800, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "73: Done 72 episodes, last 10 means: reward=-0.800, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "74: Done 73 episodes, last 10 means: reward=-0.800, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "75: Done 74 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "76: Done 75 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "77: Done 76 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "78: Done 77 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "79: Done 78 episodes, last 10 means: reward=-0.900, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "80: Done 79 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "80: nets synced, mean loss for last 10 steps = 0.800\n",
      "81: Done 80 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "82: Done 81 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "83: Done 82 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "84: Done 83 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "85: Done 84 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.09 steps/s, eps=0.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86: Done 85 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "87: Done 86 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "88: Done 87 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "89: Done 88 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "90: Done 89 episodes, last 10 means: reward=-0.901, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "90: nets synced, mean loss for last 10 steps = 0.772\n",
      "91: Done 90 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "92: Done 91 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "93: Done 92 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "94: Done 93 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "95: Done 94 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "96: Done 95 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "97: Done 96 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "98: Done 97 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "99: Done 98 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.09 steps/s, eps=0.20\n",
      "100: Done 99 episodes, last 10 means: reward=-1.000, steps=2.000, speed=0.10 steps/s, eps=0.20\n",
      "100: nets synced, mean loss for last 10 steps = 0.832\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-258f5feda2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_rewards_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/ptan/ptan/experience.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \"\"\"\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/ptan/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/ptan/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrouped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mnext_state_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-1d8fedff9be3>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mobs_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_done_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mo_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mreward_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/wrappers/timer.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpyprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnc_env.Timer.step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Calculate how much time was spent actually doing work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/wrappers/render.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/wrappers/throttle.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mpyprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnc_env.Throttle.sleep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0maccum_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stats.throttle.sleep'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;31m# We want to merge in the latest reward/done/info so that our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "losses = []\n",
    "steps = 0\n",
    "last_ts = time.time()\n",
    "last_steps = 0\n",
    "\n",
    "while True:\n",
    "    steps += 1\n",
    "    buffer.populate(1)\n",
    "    r = exp_source.pop_rewards_steps()\n",
    "    if r:\n",
    "        for rw, st in r:\n",
    "            episode_rewards.append(rw)\n",
    "            episode_steps.append(st)\n",
    "        speed = (steps - last_steps) / (time.time() - last_ts)\n",
    "        print(\"%d: Done %d episodes, last 10 means: reward=%.3f, steps=%.3f, speed=%.2f steps/s, eps=%.2f\" % (\n",
    "            steps, len(episode_rewards), np.mean(episode_rewards[-10:]), \n",
    "            np.mean(episode_steps[-10:]), speed, action_selector.epsilon\n",
    "        ))\n",
    "        last_ts = time.time()\n",
    "        last_steps = steps\n",
    "        if np.mean(episode_rewards[-10:]) > 0.9:\n",
    "            print(\"You solved the env! Congrats!\")\n",
    "            break\n",
    "    if len(buffer) < MIN_REPLAY:\n",
    "        continue\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    state_v, actions_v, ref_q_v = unpack_batch(batch, tgt_net.target_model, gamma=GAMMA, device=DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    q_v = net(state_v)\n",
    "    q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    loss_v = F.mse_loss(q_v, ref_q_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss_v.item())\n",
    "    \n",
    "    if steps % TGT_SYNC == 0:\n",
    "        print(\"%d: nets synced, mean loss for last 10 steps = %.3f\" % (\n",
    "            steps, np.mean(losses[-10:])))\n",
    "        tgt_net.sync()\n",
    "    action_selector.epsilon = max(FINAL_EPSILON, INITIAL_EPSILON - (steps - MIN_REPLAY) / STEPS_EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
