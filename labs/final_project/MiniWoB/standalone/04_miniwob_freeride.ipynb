{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting container 1\n",
      "74945919425dce3484f01a5129892be1b5e3628bd4d9d62835f1f359c3d774bb\n"
     ]
    }
   ],
   "source": [
    "!./containers_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/runtimes/__init__.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  spec = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import universe\n",
    "import collections\n",
    "from typing import List, Optional, Tuple\n",
    "from universe import vectorized\n",
    "from universe.wrappers.experimental import SoftmaxClickMouse\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKER_IMAGE = \"shmuma/miniwob:latest\"\n",
    "ENV_NAME = \"wob.mini.ClickDialog-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build connection endpoints for set of containers\n",
    "# you should tweak its args if you're not using standalone installation\n",
    "def remotes_url(port_ofs=0, hostname='localhost', count=8):\n",
    "    hosts = [\"%s:%d+%d\" % (hostname, 5900 + ofs, 15900 + ofs) for ofs in range(port_ofs, port_ofs+count)]\n",
    "    return \"vnc://\" + \",\".join(hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(wrapper_func = lambda env: env, count: int = 1, fps: float = 5) -> universe.envs.VNCEnv:\n",
    "    \"\"\"\n",
    "    Builds the vectorized env\n",
    "    \"\"\"\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = wrapper_func(env)\n",
    "    url = remotes_url(count=count)\n",
    "    print(\"Remotes URL: %s\" % url)\n",
    "\n",
    "    env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=fps, vnc_kwargs={\n",
    "            'encoding': 'tight', 'compress_level': 0,\n",
    "            'fine_quality_level': 100, 'subsample_level': 0\n",
    "        })\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_env(env: universe.envs.VNCEnv):\n",
    "    \"\"\"\n",
    "    Function performs initial reset of the env and waits for observations to become ready\n",
    "    \"\"\"\n",
    "    obs_n = env.reset()\n",
    "    while any(map(lambda o: o is None, obs_n)):\n",
    "        a = [env.action_space.sample() for _ in obs_n]\n",
    "        obs_n, reward, is_done, info = env.step(a)\n",
    "    return obs_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniWoBCropper(vectorized.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Crops the WoB area and converts the observation into PyTorch (C, H, W) format.\n",
    "    \"\"\"\n",
    "    # Area of interest\n",
    "    WIDTH = 160\n",
    "    HEIGHT = 210\n",
    "    X_OFS = 10\n",
    "    Y_OFS = 75\n",
    "    \n",
    "    def __init__(self, env, keep_text=False):\n",
    "        super(MiniWoBCropper, self).__init__(env)\n",
    "        self.keep_text = keep_text\n",
    "        img_space = gym.spaces.Box(low=0, high=255, shape=(3, self.HEIGHT, self.WIDTH))\n",
    "        if keep_text:\n",
    "            self.observation_space = gym.spaces.Tuple(spaces=(img_space, gym.spaces.Space))\n",
    "        else:\n",
    "            self.observation_space = img_space\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        res = []\n",
    "        for obs in observation_n:\n",
    "            if obs is None:\n",
    "                res.append(obs)\n",
    "                continue\n",
    "            img = obs['vision'][self.Y_OFS:self.Y_OFS+self.HEIGHT, self.X_OFS:self.X_OFS+self.WIDTH, :]\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "            if self.keep_text:\n",
    "                text = \" \".join(map(lambda d: d.get('instruction', ''), obs.get('text', [{}])))\n",
    "                res.append((img, text))\n",
    "            else:\n",
    "                res.append(img)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, 5, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast], net: nn.Module, gamma: float, device=\"cpu\"):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    last_states_v = torch.tensor(last_states).to(device)\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v + rewards_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryTracking(vectorized.Wrapper):\n",
    "    \"\"\"\n",
    "    Keeps last N trajectories from the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, env, size: int):\n",
    "        super(TrajectoryTracking, self).__init__(env)\n",
    "        self.trajectories = collections.deque(maxlen=size)\n",
    "        self._in_progress = []\n",
    "\n",
    "    def reset(self):\n",
    "        obs_n = self.env.reset()\n",
    "        if not self._in_progress:\n",
    "            self._in_progress = [[] for _ in obs_n]\n",
    "        for t, obs in zip(self._in_progress, obs_n):\n",
    "            if t:\n",
    "                self.trajectories.append(list(t))\n",
    "                t.clear()\n",
    "            if obs:\n",
    "                t.append(obs)\n",
    "        return obs_n\n",
    "    \n",
    "    def step(self, action_n):\n",
    "        obs_n, r_n, done_n, info_n = self.env.step(action_n)\n",
    "        for t, obs, r, act in zip(self._in_progress, obs_n, r_n, action_n):\n",
    "            t.append((obs, act, r))\n",
    "        for t, done in zip(self._in_progress, done_n):\n",
    "            if done:\n",
    "                self.trajectories.append(list(t))\n",
    "                t.clear()\n",
    "        return obs_n, r_n, done_n, info_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniWoBTrackClicks(vectorized.Wrapper):\n",
    "    \"\"\"\n",
    "    Track the clicks\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(MiniWoBTrackClicks, self).__init__(env)\n",
    "        s = env.observation_space.shape\n",
    "        self.clicks_buf = np.zeros(s[1:], dtype=np.uint8)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(s[0]+1, s[1], s[2]))\n",
    "\n",
    "    def reset(self):\n",
    "        self.clicks_buf[:] = 0\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action_n):\n",
    "        # TODO: implement me properly for N environments :)\n",
    "        obs_n, r_n, done_n, info_n = self.env.step(action_n)\n",
    "        if obs_n[0] is None:\n",
    "            return obs_n, r_n, done_n, info_n\n",
    "\n",
    "        # track the click\n",
    "        x = action_n[0] // 16\n",
    "        y = action_n[0] % 16 + 5    # +5 is needed to offset click region down 50 pixels\n",
    "        self.clicks_buf[y*10:(y+1)*10, x*10:(x+1)*10] = 255\n",
    "        \n",
    "        obs_n[0] = np.vstack((obs_n[0], [self.clicks_buf]))\n",
    "        if done_n[0]:\n",
    "            self.clicks_buf[:] = 0\n",
    "        return obs_n, r_n, done_n, info_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniWoBSlowpoke(vectorized.Wrapper):\n",
    "    \"\"\"\n",
    "    Limits the amount of time we can click.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, click_delay: float = 1):\n",
    "        super(MiniWoBSlowpoke, self).__init__(env)\n",
    "        self.click_delay = click_delay\n",
    "        self.click_ts = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.click_ts = None\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action_n):\n",
    "        # TODO: implement me properly for N environments :)\n",
    "        if self.click_ts is not None:\n",
    "            dt = self.click_delay - (time.time() - self.click_ts)\n",
    "            if dt > 0:\n",
    "                time.sleep(dt)\n",
    "        obs_n, r_n, done_n, info_n = self.env.step(action_n)\n",
    "        if done_n[0]:\n",
    "            self.click_ts = None\n",
    "        else:\n",
    "            self.click_ts = time.time()\n",
    "        return obs_n, r_n, done_n, info_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible directions to explore\n",
    "\n",
    "* Take different environment\n",
    "* Better exploration, for example code of NoisyNetworks [could be found here](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter07/lib/dqn_model.py#L9) \n",
    "* Try DQN tweaks, like Dueling, N-steps, Prioritized reploy buffers, [examples are here](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter07)\n",
    "* Tweak params\n",
    "* Incorporate human demonstrations into the learning process (no time to describe it here, could be found in chapter 13 of my book, [code is here](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter13))\n",
    "* Your favorite method :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
