{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ptan intro\n",
    "\n",
    "[PTAN](https://github.com/Shmuma/ptan) (abbrevation of `PyTorch AgentNet`) is a small library I wrote to simplify RL experiments. It tries to keep the balance between two extremes:\n",
    "\n",
    "1. import lib, then write one line to train the DQN (very vivid example is [OpenAI baselines project](https://github.com/openai/baselines/))\n",
    "2. implement everything from scratch\n",
    "\n",
    "First approach is very inflexible. It works good when you're using the library the way it supposed to be used. But if you want to do something fancy, you quickly find yourself hacking the lib and fighting with constraints imposed by the author rather than solving the problem you want to solve.\n",
    "\n",
    "Second extreme gives you *too much freedom* and requires implementing replay buffers and trajectory handling over and over again, which is error-prone, boring and inefficient.\n",
    "\n",
    "Several years ago I was tired of writing replay buffers and decided to implement something in between: not \"the universal RL lib\", but a set of classes to avoid writing boilerplate code.\n",
    "\n",
    "I used ptan to implement all the [examples for the \"Deep Reinforcement Learning Hands-On\" book](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/), which include all the major DRL methods, which includes DQN, A3C, all tricks in Rainbow paper, DDPG, D4PG, PPO, TRPO, Acktr and AlphaGo Zero.\n",
    "\n",
    "## High-level overview\n",
    "\n",
    "From the high level, ptan provides you the following entities:\n",
    "\n",
    "* `Agent`: class which knows how to convert batch of observations to batch of actions to be executed. It can contain optional state, in case you need to track some info between consequent actions in one episode (for example, noise params for Ornsteinâ€“Uhlenbeck exploration). Normally, you can use [already existing Agent instances](https://github.com/Shmuma/ptan/blob/master/ptan/agent.py) or write your own subclass of `BaseAgent`.\n",
    "* `ActionSelector`: small piece of logic which knows how to choose the action from some output of the network. Works in tandem with `Agent` https://github.com/Shmuma/ptan/blob/master/ptan/actions.py\n",
    "* `ExperienceSource` and variations: by using the `Agent` instance and gym environment object can provide information about the trajectory from episodes. In the simplest form it is one single $(a, r, s')$ transition at a time, but functionality goes beyond this. Source file is https://github.com/Shmuma/ptan/blob/master/ptan/experience.py\n",
    "* `ExperienceSourceBuffer` and friends: replay buffers with various characteristics. Includes simple replay buffer and two versions of prioritized replay buffers\n",
    "* various utility classes, like `TargetNet` (both discrete and continuous), wrappers for time-series preprocessing (used for tracking training progress in TensorBoard)\n",
    "* includes wrappers for Gym environments, for example, wrappers for Atari games (copy-pasted from OpenAI baselines with some tweaks): https://github.com/Shmuma/ptan/blob/master/ptan/common/wrappers.py\n",
    "\n",
    "And that's basically it. Total amount of sourse is just ~1500 lines of Python, which is almost nothing.\n",
    "\n",
    "Below I'm going to demonstrate how ptan could be used to simplify RL methods implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Installation\n",
    "\n",
    "We'll need gym, opencv python bindings. And pytorch, of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages (0.7.4)\n",
      "Requirement already satisfied: opencv_python in /home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages (4.1.0.25)\n",
      "Requirement already satisfied: six in /home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages (from gym) (1.16.4)\n",
      "Requirement already satisfied: requests>=2.0 in /home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages (from gym) (2.10.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: future in /home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n",
      "Requirement already satisfied: ptan in /home/shmuma/work/ptan (0.4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install gym opencv_python\n",
    "!pip install ptan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Action selector\n",
    "\n",
    "https://github.com/Shmuma/ptan/blob/master/ptan/actions.py\n",
    "\n",
    "Helps to go from network output to concrete action values. Most common cases:\n",
    "* Argmax: commonly used by Q-value methods, when the network predicts Q-values for set of actions and the desired action is the action with the largest Q\n",
    "* Policy-based: network outputs the probability distribution (in form of logits or normalized distribution) and action need to be sampled from this distribution. Used commonly by PG-methods\n",
    "\n",
    "Action selector is used by the `Agent`, and rarely need to be customized (but you have this option). Concrete classes which could be used:\n",
    "* [`ArgmaxActionSelector`](https://github.com/Shmuma/ptan/blob/master/ptan/actions.py#L12): applies `argmax` on the second axis of passed tensor (matrix is assumed)\n",
    "* [`ProbabilityActionSelector`](https://github.com/Shmuma/ptan/blob/master/ptan/actions.py#L36): samples from probability distribution of discrete set of actions\n",
    "* [`EpsilonGreedyActionSelector`](https://github.com/Shmuma/ptan/blob/master/ptan/actions.py#L21): has parameter $\\epsilon$ which specifies the probability of random action to be taken. \n",
    "\n",
    "All the classes assume numpy arrays to be passed to them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 1, -1,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals = np.array([[1, 2, 3], [1, -1, 0]])\n",
    "q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0)\n",
    "selector(q_vals)\n",
    "# have to be the same result, as episilon is 0 (no random actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "selector(q_vals)\n",
    "# will be random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1]\n",
      "[1 2 0]\n",
      "[2 2 0]\n",
      "[1 2 0]\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[1 2 1]\n",
      "[1 2 0]\n",
      "[1 2 0]\n",
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# here we sample from probability distribution (have to be normalized)\n",
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "for _ in range(10):\n",
    "    acts = selector(np.array([\n",
    "        [0.1, 0.8, 0.1],\n",
    "        [0.0, 0.0, 1.0],\n",
    "        [0.5, 0.5, 0.0]\n",
    "    ]))\n",
    "    print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Agent class\n",
    "\n",
    "`Agent` is class which knows how to convert observations into actions. There are three most common approaches:\n",
    "* **Q-function**: NN predicts Q-values for actions, the $argmax Q(s)$ is the action\n",
    "* **Policy-based**: NN predicts probability distribution over actions $\\pi(s)$, you sample from this distribution and get the action to do\n",
    "* **Continuous control**: NN predits the $\\mu(s)$ of continuous control parameters and the output is your actions to execute.\n",
    "\n",
    "Third case is trivial, two first approached is implemented in `ptan` to be reused without any coding: [`DQNAgent`](https://github.com/Shmuma/ptan/blob/master/ptan/agent.py#L55) and [`PolicyAgent`](https://github.com/Shmuma/ptan/blob/master/ptan/agent.py#L104).\n",
    "\n",
    "But in reality, it is often needed to implement your own agent, some of the reasons:\n",
    "* You have fancy architecture of the net -- mixture of continuous and discrete action space, have multi-modal observations (text and pixels, for example)\n",
    "* You want to use non-standard exploration strategies, for example Ornsteinâ€“Uhlenbeck process (very popular exploration strategy in continuous control domain)\n",
    "* You have PoMDP environment and you decision are not fully defined by observations, but by some internal agent state (which is also the case for Ornsteinâ€“Uhlenbeck)\n",
    "\n",
    "All those cases are easily supported by subclassing the `BaseAgent` class, in TextWorld's tutorial we'll do exactly this.\n",
    "\n",
    "Below is the example how provided `DQNAgent` and `PolicyAgent` could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## DQNAgent\n",
    "\n",
    "Suppose we have NN which produces Q-values from observations. `DQNAgent` takes batch of observations on input (as numpy array), apply the network on them to get Q-values, then uses provided `ActionSelector` to convert Q-values to indices of actions.\n",
    "\n",
    "Below is the small example. For simplicity, our network always produces the same output for the input batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.actions = actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # we always produce diagonal tensor of shape (batch_size, actions)\n",
    "        return torch.eye(x.size()[0], self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Net(actions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.zeros(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, let's use simple $argmax$ policy for the beginning. Agent will return actions corresponding to 1s in the net output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector, device=\"cpu\")\n",
    "# note that you need to tell agent are you using GPU or not by passing device, by default it equals to \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can pass the agent some observations (which will be ignored as our example is trivial), the output will be the actions according to NN output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), [None, None])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(torch.zeros(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The output from the agent is a tuple with two components:\n",
    "1. numpy array with actions to be executed -- in our case of discrete actions, they are indices\n",
    "2. list with agent's internal state. This is used for stateful agents, and is a list of None in our case. As our agent is stateless, you can ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's try to make the agent with epsilon-greedy exploration strategy. For this, we need just pass a different action selector and that's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As epsilon is 1, all the actions will be random, regardless of network's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 2, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(torch.zeros(10, 5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But we can change the epsilon value on the fly, which is very handy during the training, when we supposed to anneal epsilon over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 2, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.epsilon = 0.5\n",
    "agent(torch.zeros(10, 5))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 0, 0, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.epsilon = 0.1\n",
    "agent(torch.zeros(10, 5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## PolicyAgent\n",
    "\n",
    "`PolicyAgent` expects the network to produce policy distribution over discrete set of actions. Policy distribution could be either logits (unnormalized) or normalized distribution. In practice you should always use logits to improve stability.\n",
    "\n",
    "Let's reimplement our above sample, but now network will produce probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.actions = actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Now we produce the tensor with first two actions having the same logit scores\n",
    "        res = torch.zeros((x.size()[0], self.actions), dtype=torch.float32)\n",
    "        res[:, 0] = 1\n",
    "        res[:, 1] = 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(actions=5)\n",
    "net(torch.zeros(6, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we need to use `ProbabilityActionSelector`. Also note the agument `apply_softmax=True` which tells agent that output is not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can pass agent observations (fake, as before) and get some actions. Agent, as before returns the tuple with actions and internal state, which will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 1, 3, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(torch.zeros(6, 5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please note, that softmax returns non-zero probabilities to zero logits, so, actions 2-5 are still could be sampled (but less likely than 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience source\n",
    "\n",
    "`Agent` abstraction described above allows us to implement environment communications in a generic way. This communication is happening in form of trajectories, produced by applying agent's actions to gym environment.\n",
    "\n",
    "At high level, Experience source classes take the agent instance, environment and provide you step-by step data from the trajectories. Functionality of those clases include:\n",
    "1. support of multiple environments being communicated at the same time. This allows efficient GPU utilization as batch of observations being processed by agent at once.\n",
    "2. trajectory could be preprocessed and presented in a convenient form for further training. For example, there is implementation of sub-trajectory rollouts, which is convenient for DQN and n-step DQN, when we're not interested in intermediate steps in n-step subtrajectories, only in first and last observations + total reward for the subtrajectory.\n",
    "3. support of vectorized environments from OpenAI Universe\n",
    "\n",
    "So, the experience source classes acts as a \"magic black box\" hiding the environment interaction and trajectory handling complexities from the library user. But the overall ptan philosophy is to be flexible and extensible, so, if you want, you can subclass one of existing classes or implement your own version in case of neccessity. \n",
    "\n",
    "There are classes which are provided by the system:\n",
    "* [`ExperienceSource`](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L18): by using agent and the set of environments produces n-step subtrajectories with all intermediate steps.\n",
    "* [`ExperienceSourceFirstLast`](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L161): the same as `ExperienceSource`, but instead of full subtrajectory (with all steps) keeps only first and last steps with proper reward accumulation in between. This can save lots of memory in case of N-step DQN or A2C rollouts.\n",
    "* [`ExperienceSourceRollouts`](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L200): follows A3C rollouts scheme described in Minh's paper about Atari games.\n",
    "\n",
    "All the classes are written to be efficient both in terms of CPU and memory, which is not very important for toy problems, but might become an issue when you want to solve Atari games, keeping 10M samples in replay buffer using commodity hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy gym environment\n",
    "\n",
    "For demonstration purpoposes, we'll implement very simple gym environment with small predictable observation state to show how Experience source classes works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment with observation 0..4 and actions 0..2\n",
    "    Observations are rotated sequentialy mod 5, reward is equal to given action.\n",
    "    Episodes are having fixed length of 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ToyEnv, self).__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        self.step_index = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.step_index = 0\n",
    "        return self.step_index\n",
    "    \n",
    "    def step(self, action):\n",
    "        is_done = self.step_index == 10\n",
    "        if is_done:\n",
    "            return self.step_index % self.observation_space.n, 0.0, is_done, {}\n",
    "        self.step_index += 1\n",
    "        return self.step_index % self.observation_space.n, float(action), self.step_index == 10, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ToyEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1.0, False, {})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2.0, False, {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, False, {})\n",
      "(1, 0.0, False, {})\n",
      "(2, 0.0, False, {})\n",
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, True, {})\n",
      "(0, 0.0, True, {})\n",
      "(0, 0.0, True, {})\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    r = env.step(0)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need the agent which always generates fixed action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, action: int):\n",
    "        self.action = action\n",
    "        \n",
    "    def __call__(self, observations: List[Any], state: Optional[List] = None) -> Tuple[List[int], Optional[List]]:\n",
    "        return [self.action for _ in observations], state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1], None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DullAgent(action=1)\n",
    "agent([1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperienceSource class\n",
    "\n",
    "Generates chunks of trajectories of the given length.\n",
    "\n",
    "Constructor arguments:\n",
    "* gym environment to be use (could be the list of environments or one single environment)\n",
    "* the agent\n",
    "* `steps_count=2`: the length of sub-trajectories to be generated\n",
    "* `steps_delta=1`: step in subtrajectories\n",
    "* `vectorized=False`: if true, environment is OpenAI Universe vectorized environment (more about them in MiniWoB tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All experience source classes are providing standard python's iterator interface, so, you can just iterate over them to get sub-trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "for exp in exp_source:\n",
    "    print(exp)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a tuple of length `steps_count` (in our case we requested sub-trajectories of length 2). Every entry is a namedtuple object with the following fields:\n",
    "* state: state we observed before taking the action\n",
    "* action: action we've done\n",
    "* reward: immediate reward we've got from env\n",
    "* done: was the episode done or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=4, action=1, reward=1.0, done=True),)\n"
     ]
    }
   ],
   "source": [
    "for exp in exp_source:\n",
    "    print(exp)\n",
    "    if exp[0].done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, that partial trajectories are alse returned, so, we can handle end of episodes properly.\n",
    "\n",
    "At the end of episode, environment is being reset automatically, so, we don't need to bother about them, just keep iterating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=4, action=1, reward=1.0, done=True),)\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's very convenient, especially in cases when we have several environments running in parallel (several instances of the same Atari game, for example).\n",
    "\n",
    "Let's increase length of our experience chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=4, action=1, reward=1.0, done=True),)\n"
     ]
    }
   ],
   "source": [
    "for exp in exp_source:\n",
    "    print(exp)\n",
    "    if exp[0].done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're getting subtrajectories of length 4, including the final pieces of trajectory.\n",
    "\n",
    "Let's give several environments to the experience source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ptan.experience.ExperienceSource(env=[ToyEnv(), ToyEnv()], agent=agent, steps_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n"
     ]
    }
   ],
   "source": [
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our environments are being iterated on a round-robin fashion, giving us access to trajectories from both environment step-by-step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperienceSourceFirstLast\n",
    "\n",
    "Class `ExperienceSource` provides us full subtrajectories of given length as the list of $(s, a, r)$ objects. The next state $s'$ is returned in the next tuple, which is not always convenient. \n",
    "\n",
    "For example, in DQN training, we want to have tuples $(s, a, r, s')$ at once to do 1-step Bellman approximation during the training. In addition, some extension of DQN, like n-step DQN might want to collapse longer sequences of observations into (first-state, action, total-reward-for-n-steps, state-after-step-n).\n",
    "\n",
    "To support this in a generic way, simple subclass of `ExperienceSource` is implemented: `ExperienceSourceFirstLast`. It accepts almost the same arguments in constructor, but returns different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n"
     ]
    }
   ],
   "source": [
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it returns single object on every iteration, which is again a namedtuple with the following fields:\n",
    "* `state`: state which we used to decide on action to make\n",
    "* `action`: action we've done at this step\n",
    "* `reward`: partial accumulated reward for `steps_count` (in our case, `steps_count=1`, so it is equal to immediate reward)\n",
    "* `last_state`: the state we've got after executing the action. If our episode ends, we have None here\n",
    "\n",
    "This data is much more convenient for DQN training, as we can apply Bellman approximation directly on this data.\n",
    "\n",
    "Let's check the result with larger amount of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
      "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
      "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=0)\n",
      "ExperienceFirstLast(state=4, action=1, reward=2.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
      "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
      "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n"
     ]
    }
   ],
   "source": [
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we're collapsing two steps on every iteration, calculating immediate reward (that's why our reward=2.0 for most of the samples).\n",
    "\n",
    "More interesting samples are at the end of the episode:\n",
    "```\n",
    "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None)\n",
    "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
    "```\n",
    "\n",
    "As episode ends, we have `last_state=None` in those samples, but additionally, we calculating the tail of the episode. Those tiny details are very easy to implement wrong, if you're doing all the trajectory handling yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience source buffers\n",
    "\n",
    "In DQN we rarely dealing with immediate experience samples, as they are heavily correlated, which lead to instability in training. \n",
    "\n",
    "Normally, we have large replay buffers, which are being populated with experience pieces. Then the buffer is being sampled (randomly or with priority weights) to get the training batch. Replay buffer normally has the maximum capacity, so old samples are being pushed out when replay buffer reaches the limit.\n",
    "\n",
    "There are several implementation tricks here, which becomes extremely important when you need to deal with large problems:\n",
    "* how to efficiently sample from large buffer\n",
    "* how to push old samples from the buffer\n",
    "* in case of prioritized buffer, how priorities need to be maintained and handled in the most efficient way.\n",
    "\n",
    "All this becomes quite non-trivial task, if you want to solve atari, keeping 10-100M samples where every sample is an image from the game. Small mistake can lead to 10-100x memory increase and major slowdowns of the training process.\n",
    "\n",
    "Ptan provides several variants of replay buffers, which provide simple integration with `ExperienceSource` and `Agent` machinery. Normally, what you need to do is to ask buffer to pull new sample from the source and sample the training batch.\n",
    "\n",
    "Provided classes:\n",
    "* [`ExperienceReplayBuffer`](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L327): simple replay buffer of predefined size with uniform sampling\n",
    "* [`PrioReplayBufferNaive`](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L371): simple, but not very efficient prioritized replay buffer implementation. Complexity of sampling is O(n), which might become an issue with large buffers\n",
    "* [`PrioritizedReplayBuffer`](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L414): uses segment trees for sampling, which makes code cryptic, but with O(log(n)) sampling complexity.\n",
    "\n",
    "Below is the example of simple relay buffer, if you want, you can find examples of `PrioritizedReplayBuffer` usage in examples for chapter 7 of my book: https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter07/05_dqn_prio_replay.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All replay buffers provides the following interface:\n",
    "* python iterator interface to walk over all the samples in the buffer\n",
    "* method `populate(N)`, to get N samples from the experience source and put into the buffer\n",
    "* method `sample(N)`, to get the batch of N experience objects\n",
    "\n",
    "So, the normal training loop for DQN looks like infinite repetition of the following steps:\n",
    "1. call `buffer.populate(1)` to get fresh sample from the environment\n",
    "2. `batch = buffer.sample(BATCH_SIZE)` to get the batch from buffer\n",
    "3. calculate the loss on the sampled batch\n",
    "4. backpropagate\n",
    "5. repeat until convergence (hopefully)\n",
    "\n",
    "All the rest is happening automatically -- reset of the environment, sub-trajectories handling, buffer size maintenance, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n"
     ]
    }
   ],
   "source": [
    "for step in range(10):\n",
    "    buffer.populate(1)\n",
    "    # if buffer is small enough, do nothing\n",
    "    if len(buffer) < 5:\n",
    "        continue\n",
    "    batch = buffer.sample(4)\n",
    "    print(\"Train time, %d batch samples:\" % len(batch))\n",
    "    for s in batch:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring the training\n",
    "\n",
    "Normally, if we're running the training process, we want to keep an eye on several metrics to check how good our method is doing. Minimal set of things to watch includes:\n",
    "* training loss (several loss components in case of A2C, for example)\n",
    "* values predicted by the network (in case of DQN)\n",
    "* statistics about episode rewards (to check that our agent improves over time)\n",
    "* statistics about the length of the episode, as this is normally a proxy for reward\n",
    "\n",
    "First two items are being calculated in the training loop, but the rest two values are not that easy to get. If we're implementing everything from scratch, we need to track the current episode and when it ends, track the total reward and length.\n",
    "\n",
    "Ptan simplifies this by providing the method in experience source, which returns this information in one call. Method `pop_rewards_steps()` returns the list, where each entry is the information about the episode which since the lass call to the method. If no episodes have completed between the calls, empty list is returned. \n",
    "\n",
    "Every item is a tuple with (total_reword, total_steps). \n",
    "\n",
    "So, the only thing you need to do to monitor the training progress, is to periodically call method `pop_rewards_steps()` in the training loop and handle the results (printing on console or sending to TensorBoard, or whatever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = exp_source.pop_rewards_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10.0, 10)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've one one episode completed so far, it got reward 10.0 and total amount of steps was 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_source.pop_rewards_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tools\n",
    "\n",
    "There are several smaller things, which could be used, like [`TargetNet`](https://github.com/Shmuma/ptan/blob/master/ptan/agent.py#L79), which allows you to keep a copy of model weights and syncronize them from time to time (which is essential for DQN to converge), or a [set of utils](https://github.com/Shmuma/ptan/blob/master/ptan/common/utils.py) to smooth time series for better training progress visualisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CartPole solver\n",
    "\n",
    "Below is very simple DQN version which solves CartPole, just to demonstrate how all things fits together in real life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CartPole is stupid -- they return double observations, rather than standard floats, so, the cast here\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "REPLAY_SIZE = 1000\n",
    "LR = 1e-3\n",
    "GAMMA=0.9\n",
    "EPS_DECAY=0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-11 16:00:23,129] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(obs_size=env.observation_space.shape[0], hidden_size=64, n_actions=env.action_space.n)\n",
    "optimizer = optim.Adam(net.parameters(), LR)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "agent = ptan.agent.DQNAgent(net, action_selector)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast], net: nn.Module, gamma: float):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.tensor(states)\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards)\n",
    "    last_states_v = torch.tensor(last_states)\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v + rewards_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: episode done, reward=15.000, steps=15, epsilon=1.00\n",
      "27: episode done, reward=11.000, steps=11, epsilon=1.00\n",
      "42: episode done, reward=15.000, steps=15, epsilon=1.00\n",
      "55: episode done, reward=13.000, steps=13, epsilon=1.00\n",
      "67: episode done, reward=12.000, steps=12, epsilon=1.00\n",
      "88: episode done, reward=21.000, steps=21, epsilon=1.00\n",
      "120: episode done, reward=32.000, steps=32, epsilon=1.00\n",
      "156: episode done, reward=36.000, steps=36, epsilon=1.00\n",
      "179: episode done, reward=23.000, steps=23, epsilon=1.00\n",
      "203: episode done, reward=24.000, steps=24, epsilon=0.99\n",
      "226: episode done, reward=23.000, steps=23, epsilon=0.88\n",
      "248: episode done, reward=22.000, steps=22, epsilon=0.79\n",
      "267: episode done, reward=19.000, steps=19, epsilon=0.71\n",
      "283: episode done, reward=16.000, steps=16, epsilon=0.66\n",
      "302: episode done, reward=19.000, steps=19, epsilon=0.60\n",
      "313: episode done, reward=11.000, steps=11, epsilon=0.57\n",
      "323: episode done, reward=10.000, steps=10, epsilon=0.54\n",
      "339: episode done, reward=16.000, steps=16, epsilon=0.50\n",
      "353: episode done, reward=14.000, steps=14, epsilon=0.46\n",
      "365: episode done, reward=12.000, steps=12, epsilon=0.44\n",
      "376: episode done, reward=11.000, steps=11, epsilon=0.41\n",
      "386: episode done, reward=10.000, steps=10, epsilon=0.39\n",
      "394: episode done, reward=8.000, steps=8, epsilon=0.38\n",
      "403: episode done, reward=9.000, steps=9, epsilon=0.36\n",
      "413: episode done, reward=10.000, steps=10, epsilon=0.34\n",
      "424: episode done, reward=11.000, steps=11, epsilon=0.33\n",
      "434: episode done, reward=10.000, steps=10, epsilon=0.31\n",
      "446: episode done, reward=12.000, steps=12, epsilon=0.29\n",
      "455: episode done, reward=9.000, steps=9, epsilon=0.28\n",
      "474: episode done, reward=19.000, steps=19, epsilon=0.25\n",
      "486: episode done, reward=12.000, steps=12, epsilon=0.24\n",
      "494: episode done, reward=8.000, steps=8, epsilon=0.23\n",
      "506: episode done, reward=12.000, steps=12, epsilon=0.22\n",
      "516: episode done, reward=10.000, steps=10, epsilon=0.21\n",
      "526: episode done, reward=10.000, steps=10, epsilon=0.20\n",
      "536: episode done, reward=10.000, steps=10, epsilon=0.19\n",
      "545: episode done, reward=9.000, steps=9, epsilon=0.18\n",
      "554: episode done, reward=9.000, steps=9, epsilon=0.17\n",
      "563: episode done, reward=9.000, steps=9, epsilon=0.16\n",
      "573: episode done, reward=10.000, steps=10, epsilon=0.15\n",
      "583: episode done, reward=10.000, steps=10, epsilon=0.15\n",
      "593: episode done, reward=10.000, steps=10, epsilon=0.14\n",
      "603: episode done, reward=10.000, steps=10, epsilon=0.13\n",
      "613: episode done, reward=10.000, steps=10, epsilon=0.13\n",
      "622: episode done, reward=9.000, steps=9, epsilon=0.12\n",
      "633: episode done, reward=11.000, steps=11, epsilon=0.11\n",
      "643: episode done, reward=10.000, steps=10, epsilon=0.11\n",
      "656: episode done, reward=13.000, steps=13, epsilon=0.10\n",
      "664: episode done, reward=8.000, steps=8, epsilon=0.10\n",
      "673: episode done, reward=9.000, steps=9, epsilon=0.09\n",
      "683: episode done, reward=10.000, steps=10, epsilon=0.09\n",
      "691: episode done, reward=8.000, steps=8, epsilon=0.09\n",
      "704: episode done, reward=13.000, steps=13, epsilon=0.08\n",
      "716: episode done, reward=12.000, steps=12, epsilon=0.08\n",
      "724: episode done, reward=8.000, steps=8, epsilon=0.07\n",
      "734: episode done, reward=10.000, steps=10, epsilon=0.07\n",
      "743: episode done, reward=9.000, steps=9, epsilon=0.07\n",
      "753: episode done, reward=10.000, steps=10, epsilon=0.06\n",
      "762: episode done, reward=9.000, steps=9, epsilon=0.06\n",
      "772: episode done, reward=10.000, steps=10, epsilon=0.06\n",
      "783: episode done, reward=11.000, steps=11, epsilon=0.05\n",
      "791: episode done, reward=8.000, steps=8, epsilon=0.05\n",
      "801: episode done, reward=10.000, steps=10, epsilon=0.05\n",
      "810: episode done, reward=9.000, steps=9, epsilon=0.05\n",
      "819: episode done, reward=9.000, steps=9, epsilon=0.04\n",
      "828: episode done, reward=9.000, steps=9, epsilon=0.04\n",
      "836: episode done, reward=8.000, steps=8, epsilon=0.04\n",
      "848: episode done, reward=12.000, steps=12, epsilon=0.04\n",
      "896: episode done, reward=48.000, steps=48, epsilon=0.03\n",
      "924: episode done, reward=28.000, steps=28, epsilon=0.03\n",
      "1005: episode done, reward=81.000, steps=81, epsilon=0.02\n",
      "1101: episode done, reward=96.000, steps=96, epsilon=0.01\n",
      "1128: episode done, reward=27.000, steps=27, epsilon=0.01\n",
      "1176: episode done, reward=48.000, steps=48, epsilon=0.01\n",
      "1211: episode done, reward=35.000, steps=35, epsilon=0.01\n",
      "1252: episode done, reward=41.000, steps=41, epsilon=0.01\n",
      "1282: episode done, reward=30.000, steps=30, epsilon=0.00\n",
      "1325: episode done, reward=43.000, steps=43, epsilon=0.00\n",
      "1376: episode done, reward=51.000, steps=51, epsilon=0.00\n",
      "1407: episode done, reward=31.000, steps=31, epsilon=0.00\n",
      "1444: episode done, reward=37.000, steps=37, epsilon=0.00\n",
      "1486: episode done, reward=42.000, steps=42, epsilon=0.00\n",
      "1516: episode done, reward=30.000, steps=30, epsilon=0.00\n",
      "1560: episode done, reward=44.000, steps=44, epsilon=0.00\n",
      "1610: episode done, reward=50.000, steps=50, epsilon=0.00\n",
      "1656: episode done, reward=46.000, steps=46, epsilon=0.00\n",
      "1692: episode done, reward=36.000, steps=36, epsilon=0.00\n",
      "1745: episode done, reward=53.000, steps=53, epsilon=0.00\n",
      "1809: episode done, reward=64.000, steps=64, epsilon=0.00\n",
      "1904: episode done, reward=95.000, steps=95, epsilon=0.00\n",
      "1997: episode done, reward=93.000, steps=93, epsilon=0.00\n",
      "2044: episode done, reward=47.000, steps=47, epsilon=0.00\n",
      "2132: episode done, reward=88.000, steps=88, epsilon=0.00\n",
      "2175: episode done, reward=43.000, steps=43, epsilon=0.00\n",
      "2249: episode done, reward=74.000, steps=74, epsilon=0.00\n",
      "2329: episode done, reward=80.000, steps=80, epsilon=0.00\n",
      "2358: episode done, reward=29.000, steps=29, epsilon=0.00\n",
      "2411: episode done, reward=53.000, steps=53, epsilon=0.00\n",
      "2452: episode done, reward=41.000, steps=41, epsilon=0.00\n",
      "2481: episode done, reward=29.000, steps=29, epsilon=0.00\n",
      "2528: episode done, reward=47.000, steps=47, epsilon=0.00\n",
      "2562: episode done, reward=34.000, steps=34, epsilon=0.00\n",
      "2594: episode done, reward=32.000, steps=32, epsilon=0.00\n",
      "2619: episode done, reward=25.000, steps=25, epsilon=0.00\n",
      "2732: episode done, reward=113.000, steps=113, epsilon=0.00\n",
      "2761: episode done, reward=29.000, steps=29, epsilon=0.00\n",
      "2812: episode done, reward=51.000, steps=51, epsilon=0.00\n",
      "2866: episode done, reward=54.000, steps=54, epsilon=0.00\n",
      "2918: episode done, reward=52.000, steps=52, epsilon=0.00\n",
      "2967: episode done, reward=49.000, steps=49, epsilon=0.00\n",
      "3001: episode done, reward=34.000, steps=34, epsilon=0.00\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "while True:\n",
    "    step += 1\n",
    "    buffer.populate(1)\n",
    "    solved = False\n",
    "    for reward, steps in exp_source.pop_rewards_steps():\n",
    "        print(\"%d: episode done, reward=%.3f, steps=%d, epsilon=%.2f\" % (\n",
    "            step, reward, steps, action_selector.epsilon))\n",
    "        rewards.append(reward)\n",
    "        solved = reward > 150\n",
    "    if solved:\n",
    "        print(\"Congrats!\")\n",
    "        break\n",
    "    if len(buffer) < 200:\n",
    "        continue\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    states_v, actions_v, tgt_q_v = unpack_batch(batch, net, GAMMA)\n",
    "    optimizer.zero_grad()\n",
    "    q_v = net(states_v)\n",
    "    q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    loss_v = F.mse_loss(q_v, tgt_q_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()    \n",
    "    losses.append(loss_v.item())\n",
    "    action_selector.epsilon *= EPS_DECAY\n",
    "    if step > 3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, hyperparams should be tuned, target network will improve stability, but you've got the idea :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
