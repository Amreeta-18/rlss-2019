{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting container 1\n",
      "0f137ef7faacec3cb819da25403630cac50d0147bf4219b7e8fc68668e225ccf\n"
     ]
    }
   ],
   "source": [
    "!./containers_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import universe\n",
    "from typing import List, Optional, Tuple\n",
    "from universe import vectorized\n",
    "from universe.wrappers.experimental import SoftmaxClickMouse\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKER_IMAGE = \"shmuma/miniwob:latest\"\n",
    "ENV_NAME = \"wob.mini.ClickDialog-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build connection endpoints for set of containers\n",
    "# you should tweak its args if you're not using standalone installation\n",
    "def remotes_url(port_ofs=0, hostname='localhost', count=8):\n",
    "    hosts = [\"%s:%d+%d\" % (hostname, 5900 + ofs, 15900 + ofs) for ofs in range(port_ofs, port_ofs+count)]\n",
    "    return \"vnc://\" + \",\".join(hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(wrapper_func = lambda env: env, count: int = 1, fps: float = 5) -> universe.envs.VNCEnv:\n",
    "    \"\"\"\n",
    "    Builds the vectorized env\n",
    "    \"\"\"\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = wrapper_func(env)\n",
    "    url = remotes_url(count=count)\n",
    "    print(\"Remotes URL: %s\" % url)\n",
    "\n",
    "    env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=fps, vnc_kwargs={\n",
    "            'encoding': 'tight', 'compress_level': 0,\n",
    "            'fine_quality_level': 100, 'subsample_level': 0\n",
    "        })\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_env(env: universe.envs.VNCEnv):\n",
    "    \"\"\"\n",
    "    Function performs initial reset of the env and waits for observations to become ready\n",
    "    \"\"\"\n",
    "    obs_n = env.reset()\n",
    "    while any(map(lambda o: o is None, obs_n)):\n",
    "        a = [env.action_space.sample() for _ in obs_n]\n",
    "        obs_n, reward, is_done, info = env.step(a)\n",
    "    return obs_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniWoBCropper(vectorized.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Crops the WoB area and converts the observation into PyTorch (C, H, W) format.\n",
    "    \"\"\"\n",
    "    # Area of interest\n",
    "    WIDTH = 160\n",
    "    HEIGHT = 210\n",
    "    X_OFS = 10\n",
    "    Y_OFS = 75\n",
    "    \n",
    "    def __init__(self, env, keep_text=False):\n",
    "        super(MiniWoBCropper, self).__init__(env)\n",
    "        self.keep_text = keep_text\n",
    "        img_space = gym.spaces.Box(low=0, high=255, shape=(3, self.HEIGHT, self.WIDTH))\n",
    "        if keep_text:\n",
    "            self.observation_space = gym.spaces.Tuple(spaces=(img_space, gym.spaces.Space))\n",
    "        else:\n",
    "            self.observation_space = img_space\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        res = []\n",
    "        for obs in observation_n:\n",
    "            if obs is None:\n",
    "                res.append(obs)\n",
    "                continue\n",
    "            img = obs['vision'][self.Y_OFS:self.Y_OFS+self.HEIGHT, self.X_OFS:self.X_OFS+self.WIDTH, :]\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "            if self.keep_text:\n",
    "                text = \" \".join(map(lambda d: d.get('instruction', ''), obs.get('text', [{}])))\n",
    "                res.append((img, text))\n",
    "            else:\n",
    "                res.append(img)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DQN baseline\n",
    "\n",
    "With all those simplifications done, we can implement basic DQN agent to solve some environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "Below is the model we're going to use. Nothing fancy, just two convolution layers followed by single FC-layer, returning Q-values for our 256 click locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, 5, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch preparation (practice)\n",
    "\n",
    "For data gathering we'll use [`ptan` library](https://github.com/shmuma/ptan/) which is a very thin RL-specific wrapper around Gym. It implement replay buffers, agent logic and other functions frequently required for RL training.\n",
    "\n",
    "Our first warm-up problem will be to write conversion of batch sampled from Replay Buffer into PyTorch tensors suitable for training.\n",
    "\n",
    "Input to the function is a list of `namedtuple` with the following fields ([source code](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L155)):\n",
    "* `state`: state `s` in the trajectory, which is a 160x210 array of bytes with shape \\[3, 210, 160\\]\n",
    "* `action`: action executed, which is an integer from 0 to 255 indicating the square in our click grid\n",
    "* `reward`: immediate reward obtained after action execution\n",
    "* `last_state`: state we've got after execution of action. It equals to `None` if episode finished after action.\n",
    "\n",
    "We'll use 1-step DQN, but Ptan can do N-step calculation for you. You can experiment with the difference.\n",
    "\n",
    "The output should be three tensors containing the batch data with:\n",
    "* `state`: in a form of tensor with shape \\[X, Y, ...\\]\n",
    "* `actions`: as a long tensor\n",
    "* `ref values`: approximated Q-values using Bellman equation\n",
    "\n",
    "$Q_{s,a} = r_{s,a} + \\gamma \\cdot max_{a'}Q_{s',a'}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast], net: nn.Module, gamma: float, device=\"cpu\"):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []    # list of booleans, True if this is the last step in the episode\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        # unpack every experience entry into individual lists\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # convert everything to tensors (do not forget to put them into proper devices!)\n",
    "    # apply network and find best Q values of every action (torch.max() is the right function to use (c: )\n",
    "    # zero out q-values according to done_masks\n",
    "    # return the result as tuple of three things: states tensor, actions tensor and \n",
    "    # Q-values approximation tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for your solution\n",
    "\n",
    "There are several test cases you can use to check your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 11:13:21,662] Making new env: wob.mini.ClickDialog-v0\n",
      "/home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "[2019-07-10 11:13:21,671] Using SoftmaxClickMouse with action_region=(10, 125, 170, 285), noclick_regions=[]\n",
      "[2019-07-10 11:13:21,672] SoftmaxClickMouse noclick regions removed 0 of 256 actions\n",
      "[2019-07-10 11:13:21,673] Writing logs to file: /tmp/universe-8120.log\n",
      "[2019-07-10 11:13:21,678] Using the golang VNC implementation\n",
      "[2019-07-10 11:13:21,678] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-07-10 11:13:21,698] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remotes URL: vnc://localhost:5900+15900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 11:13:38,037] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-07-10 11:13:39,360] [0:localhost:5900] Initial reset complete: episode_id=24\n"
     ]
    }
   ],
   "source": [
    "env = make_env(wrapper_func = lambda env: SoftmaxClickMouse(MiniWoBCropper(env)))\n",
    "join_env(env);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(5, 5))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=19200, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code also should help you to understand the expectations from your code\n",
    "net = Model(env.observation_space.shape, env.action_space.n)\n",
    "# with eps=1, we don't need parent action selector, as all actions will be random, but that's a corner case!\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1) \n",
    "agent = ptan.agent.DQNAgent(net, action_selector)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 11:14:06,213] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, vectorized=True)\n",
    "batch = [e for _, e in zip(range(5), exp_source)]\n",
    "# add one final state with known reward\n",
    "batch.append(ptan.experience.ExperienceFirstLast(state=batch[0].state, action=0, reward=10.0, last_state=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = unpack_batch(batch, net, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(r, tuple)\n",
    "assert len(r) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_q = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(states, torch.Tensor)\n",
    "assert states.dtype == torch.uint8\n",
    "assert states.size() == (6, 3, 210, 160)\n",
    "\n",
    "assert isinstance(actions, torch.Tensor)\n",
    "assert actions.dtype == torch.long\n",
    "assert actions.size() == (6, )\n",
    "\n",
    "assert isinstance(next_q, torch.Tensor)\n",
    "assert next_q.dtype == torch.float32\n",
    "assert next_q.size() == (6, )\n",
    "\n",
    "assert actions[5] == 0\n",
    "assert next_q[5] == 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batch preparation (solution)\n",
    "\n",
    "Please do not peek inside this, as you'll spoil all the fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast], net: nn.Module, gamma: float, device=\"cpu\"):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    last_states_v = torch.tensor(last_states).to(device)\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v + rewards_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Now we're ready to implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "REPLAY_SIZE = 10000\n",
    "MIN_REPLAY = 100\n",
    "TGT_SYNC = 100\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "DEVICE = \"cuda\"  \n",
    "#DEVICE = \"cpu\"\n",
    "\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.2\n",
    "STEPS_EPSILON = 1000\n",
    "LIMIT_STEPS = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:26:03,110] Making new env: wob.mini.ClickDialog-v0\n",
      "[2019-07-10 13:26:03,115] Using SoftmaxClickMouse with action_region=(10, 125, 170, 285), noclick_regions=[]\n",
      "[2019-07-10 13:26:03,115] SoftmaxClickMouse noclick regions removed 0 of 256 actions\n",
      "[2019-07-10 13:26:03,116] Using the golang VNC implementation\n",
      "[2019-07-10 13:26:03,117] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-07-10 13:26:03,137] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remotes URL: vnc://localhost:5900+15900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:26:19,536] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-07-10 13:26:19,569] [0:localhost:5900] Initial reset complete: episode_id=930\n"
     ]
    }
   ],
   "source": [
    "env = make_env(wrapper_func = lambda env: SoftmaxClickMouse(MiniWoBCropper(env)))\n",
    "join_env(env);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "parent_selector = ptan.actions.ArgmaxActionSelector()\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=INITIAL_EPSILON, selector=parent_selector) \n",
    "agent = ptan.agent.DQNAgent(net, action_selector, device=DEVICE)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, vectorized=True)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "optimizer = optim.Adam(net.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:26:19,902] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: Done 1 episodes, last 10 means: reward=-1.000, steps=30.000, speed=4.54 steps/s, eps=1.00\n",
      "82: Done 2 episodes, last 10 means: reward=-1.000, steps=41.500, speed=4.90 steps/s, eps=1.00\n",
      "100: nets synced, mean loss for last 10 steps = 0.163\n",
      "127: Done 3 episodes, last 10 means: reward=-0.650, steps=43.000, speed=4.56 steps/s, eps=0.97\n",
      "144: Done 4 episodes, last 10 means: reward=-0.328, steps=36.750, speed=3.98 steps/s, eps=0.96\n",
      "152: Done 5 episodes, last 10 means: reward=-0.096, steps=31.200, speed=3.89 steps/s, eps=0.95\n",
      "198: Done 6 episodes, last 10 means: reward=-0.246, steps=33.833, speed=4.40 steps/s, eps=0.90\n",
      "200: nets synced, mean loss for last 10 steps = 0.031\n",
      "245: Done 7 episodes, last 10 means: reward=-0.354, steps=35.857, speed=4.45 steps/s, eps=0.86\n",
      "291: Done 8 episodes, last 10 means: reward=-0.435, steps=37.250, speed=4.45 steps/s, eps=0.81\n",
      "300: nets synced, mean loss for last 10 steps = 0.037\n",
      "317: Done 9 episodes, last 10 means: reward=-0.334, steps=36.111, speed=4.26 steps/s, eps=0.78\n",
      "363: Done 10 episodes, last 10 means: reward=-0.400, steps=37.200, speed=4.46 steps/s, eps=0.74\n",
      "400: nets synced, mean loss for last 10 steps = 0.021\n",
      "409: Done 11 episodes, last 10 means: reward=-0.400, steps=38.900, speed=4.36 steps/s, eps=0.69\n",
      "438: Done 12 episodes, last 10 means: reward=-0.263, steps=36.600, speed=4.35 steps/s, eps=0.66\n",
      "485: Done 13 episodes, last 10 means: reward=-0.368, steps=36.800, speed=4.48 steps/s, eps=0.62\n",
      "500: nets synced, mean loss for last 10 steps = 0.076\n",
      "532: Done 14 episodes, last 10 means: reward=-0.532, steps=39.800, speed=4.45 steps/s, eps=0.57\n",
      "579: Done 15 episodes, last 10 means: reward=-0.715, steps=43.700, speed=4.50 steps/s, eps=0.52\n",
      "600: nets synced, mean loss for last 10 steps = 0.121\n",
      "627: Done 16 episodes, last 10 means: reward=-0.715, steps=43.900, speed=4.45 steps/s, eps=0.47\n",
      "671: Done 17 episodes, last 10 means: reward=-0.715, steps=43.600, speed=4.30 steps/s, eps=0.43\n",
      "700: nets synced, mean loss for last 10 steps = 0.149\n",
      "701: best reward updated: None -> -0.715, weights saved\n",
      "717: Done 18 episodes, last 10 means: reward=-0.612, steps=43.600, speed=4.44 steps/s, eps=0.38\n",
      "717: best reward updated: -0.7151500000000001 -> -0.612, weights saved\n",
      "762: Done 19 episodes, last 10 means: reward=-0.759, steps=45.500, speed=4.36 steps/s, eps=0.34\n",
      "769: Done 20 episodes, last 10 means: reward=-0.659, steps=41.600, speed=3.79 steps/s, eps=0.33\n",
      "800: nets synced, mean loss for last 10 steps = 0.411\n",
      "816: Done 21 episodes, last 10 means: reward=-0.659, steps=41.700, speed=4.41 steps/s, eps=0.29\n",
      "861: Done 22 episodes, last 10 means: reward=-0.797, steps=43.300, speed=4.33 steps/s, eps=0.24\n",
      "900: nets synced, mean loss for last 10 steps = 0.235\n",
      "908: Done 23 episodes, last 10 means: reward=-0.797, steps=43.300, speed=4.43 steps/s, eps=0.20\n",
      "955: Done 24 episodes, last 10 means: reward=-0.797, steps=43.300, speed=4.56 steps/s, eps=0.20\n",
      "963: Done 25 episodes, last 10 means: reward=-0.611, steps=39.400, speed=3.99 steps/s, eps=0.20\n",
      "963: best reward updated: -0.6119000000000001 -> -0.611, weights saved\n",
      "1000: nets synced, mean loss for last 10 steps = 0.767\n",
      "1010: Done 26 episodes, last 10 means: reward=-0.611, steps=39.300, speed=4.49 steps/s, eps=0.20\n",
      "1056: Done 27 episodes, last 10 means: reward=-0.611, steps=39.500, speed=4.38 steps/s, eps=0.20\n",
      "1100: nets synced, mean loss for last 10 steps = 0.771\n",
      "1101: Done 28 episodes, last 10 means: reward=-0.612, steps=39.400, speed=4.32 steps/s, eps=0.20\n",
      "1147: Done 29 episodes, last 10 means: reward=-0.612, steps=39.500, speed=4.44 steps/s, eps=0.20\n",
      "1193: Done 30 episodes, last 10 means: reward=-0.712, steps=43.400, speed=4.40 steps/s, eps=0.20\n",
      "1200: nets synced, mean loss for last 10 steps = 1.203\n",
      "1240: Done 31 episodes, last 10 means: reward=-0.712, steps=43.400, speed=4.43 steps/s, eps=0.20\n",
      "1287: Done 32 episodes, last 10 means: reward=-0.712, steps=43.600, speed=4.46 steps/s, eps=0.20\n",
      "1300: nets synced, mean loss for last 10 steps = 0.997\n",
      "1334: Done 33 episodes, last 10 means: reward=-0.712, steps=43.600, speed=4.49 steps/s, eps=0.20\n",
      "1381: Done 34 episodes, last 10 means: reward=-0.712, steps=43.600, speed=4.41 steps/s, eps=0.20\n",
      "1400: nets synced, mean loss for last 10 steps = 3.070\n",
      "1427: Done 35 episodes, last 10 means: reward=-0.897, steps=47.400, speed=4.42 steps/s, eps=0.20\n",
      "1473: Done 36 episodes, last 10 means: reward=-0.897, steps=47.300, speed=4.39 steps/s, eps=0.20\n",
      "1500: nets synced, mean loss for last 10 steps = 6.652\n"
     ]
    }
   ],
   "source": [
    "best_weights = None\n",
    "best_mean10_reward = None\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "losses = []\n",
    "steps = 0\n",
    "last_ts = time.time()\n",
    "last_steps = 0\n",
    "\n",
    "while steps < LIMIT_STEPS:\n",
    "    steps += 1\n",
    "    buffer.populate(1)\n",
    "    r = exp_source.pop_rewards_steps()\n",
    "    if r:\n",
    "        for rw, st in r:\n",
    "            episode_rewards.append(rw)\n",
    "            episode_steps.append(st)\n",
    "        speed = (steps - last_steps) / (time.time() - last_ts)\n",
    "        print(\"%d: Done %d episodes, last 10 means: reward=%.3f, steps=%.3f, speed=%.2f steps/s, eps=%.2f\" % (\n",
    "            steps, len(episode_rewards), np.mean(episode_rewards[-10:]), \n",
    "            np.mean(episode_steps[-10:]), speed, action_selector.epsilon\n",
    "        ))\n",
    "        last_ts = time.time()\n",
    "        last_steps = steps\n",
    "        if np.mean(episode_rewards[-10:]) > 0.9:\n",
    "            print(\"You solved the env! Congrats!\")\n",
    "            break\n",
    "    if len(buffer) < MIN_REPLAY:\n",
    "        continue\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    state_v, actions_v, ref_q_v = unpack_batch(batch, tgt_net.target_model, gamma=GAMMA, device=DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    q_v = net(state_v)\n",
    "    q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    loss_v = F.mse_loss(q_v, ref_q_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss_v.item())\n",
    "    \n",
    "    if steps % TGT_SYNC == 0:\n",
    "        print(\"%d: nets synced, mean loss for last 10 steps = %.3f\" % (\n",
    "            steps, np.mean(losses[-10:])))\n",
    "        tgt_net.sync()\n",
    "    action_selector.epsilon = max(FINAL_EPSILON, INITIAL_EPSILON - (steps-MIN_REPLAY) / STEPS_EPSILON)\n",
    "    \n",
    "    if action_selector.epsilon < 0.4:\n",
    "        m10_reward = np.mean(episode_rewards[-10:])\n",
    "        if best_mean10_reward is None or m10_reward > best_mean10_reward:\n",
    "            print(\"%d: best reward updated: %s -> %.3f, weights saved\" % (\n",
    "                steps, best_mean10_reward, m10_reward\n",
    "            ))\n",
    "            best_mean10_reward = m10_reward\n",
    "            # deep copy is required as state_dict() is just reference to tensors which are being kept on GPU\n",
    "            best_weights = copy.deepcopy(net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look good:\n",
    "* convergence is bad, \n",
    "* loss is growing\n",
    "\n",
    "Let's test the best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:33:49,821] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-07-10 13:33:58,236] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 36 steps with -1.000 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:34:08,653] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 51 steps with -1.000 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:34:19,269] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 52 steps with -1.000 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:34:29,686] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 51 steps with -1.000 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:34:30,487] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 3 steps with 0.982 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:34:40,901] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 51 steps with -1.000 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:34:51,518] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 52 steps with -1.000 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:35:01,934] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 51 steps with -1.000 reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-10 13:35:12,351] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 51 steps with -1.000 reward\n",
      "Episode done in 52 steps with -1.000 reward\n",
      "Mean reward for 10 episodes is -0.802, steps 45.000\n"
     ]
    }
   ],
   "source": [
    "# testing the best weights\n",
    "net.load_state_dict(best_weights)\n",
    "TEST_EPISODES = 10\n",
    "\n",
    "rewards = []\n",
    "steps = []\n",
    "while len(rewards) < TEST_EPISODES:\n",
    "    cur_reward = 0\n",
    "    cur_steps = 0\n",
    "    obs_n = join_env(env)\n",
    "    while True:\n",
    "        obs_t = torch.tensor(obs_n).to(DEVICE)\n",
    "        q_t = net(obs_t)\n",
    "        action = q_t.max(dim=1).indices[0].item()\n",
    "        obs_n, reward_n, is_done_n, _ = env.step([action])\n",
    "        cur_reward += reward_n[0]\n",
    "        cur_steps += 1\n",
    "        if is_done_n[0]:\n",
    "            break\n",
    "    rewards.append(cur_reward)\n",
    "    steps.append(cur_steps)\n",
    "    print(\"Episode done in %d steps with %.3f reward\" % (cur_steps, cur_reward))\n",
    "print(\"Mean reward for %d episodes is %.3f, steps %.3f\" % (TEST_EPISODES, np.mean(rewards), np.mean(steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting of Baseline DQN\n",
    "\n",
    "> \"You love Linux, probably you're experienced in troubleshooting...\"\n",
    "> RHCE Exam Instructor @ 2005\n",
    "\n",
    "Code above doesn't converge. Usually, in such situations, tutorial author tweaks the parameters to make it working, so, attendants see only final, polished version. This makes the false impression of ML as smooth and quite obvious process, which very far from truth. Don't know about others, but my code doesn't work 90% of the time :).\n",
    "\n",
    "To learn how to deal with such situations, you're asked to troubleshoot the code above. There are several directions you could explore (of course, you can have your own ideas, that's not the complete list):\n",
    "\n",
    "* check training samples. Do they make sense? Is reward properly assigned to correct observations?\n",
    "* training gradients. How large they are? What's the ratio of gradients/weight values?\n",
    "* do end-of-episode steps handled properly? Final steps of epsiodes act as an anchor to prevent Q values from growing infinitely, so, it is critical to have them properly handled in Bellman equation\n",
    "* explore Q values produced by the network during the training. Are they growing over time? That's a good practice to have small set of states (including final step and steps before the final) and track their Qs during the training.\n",
    "* how large the difference between trained network and target net used for next-step Bellman approximation? How it changes over time?\n",
    "\n",
    "Full check might take a long time, especially if you haven't done it before. In the next notebook (03_miniwob_troubleshooting) you will be given detailed steps of troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratchplace for you to enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not forget to stop container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24f0ebbed615\r\n"
     ]
    }
   ],
   "source": [
    "!./containers_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
