{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MiniWoB intro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Mini World of Bits (MiniWoB) is an RL benchmark introduced by OpenAI researchers in 2017.\n",
    "\n",
    "Their original paper http://proceedings.mlr.press/v70/shi17a/shi17a.pdf\n",
    "\n",
    "The core idea is to create a set of browser-based tasks to be solved using RL methods. Every task is a small dynamic webpage, which could be interacted using a mouse or keyboard. The reward is given for executing correct sequence of actions. Description of the goal is included into the webpage.\n",
    "\n",
    "In total it introduced 80 problems of varying complexity -- from the trivial like clicking the form button to very challenging, for example booking the flight following the criterias.\n",
    "\n",
    "The problems are available here: https://stanfordnlp.github.io/miniwob-plusplus/\n",
    "\n",
    "Unfortunately, OpenAI discontinued MiniWoB project, so, it hasn't gained popularity it deserves. After OpenAI paper in 2017, MiniWoB was used in several research papers, the most notable ones:\n",
    "\n",
    "* [1802.08802 Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration](https://arxiv.org/abs/1802.08802)\n",
    "* [1812.09195 Learning to Navigate the Web](https://arxiv.org/abs/1812.09195v1)\n",
    "* [1902.07257v1 DOM-Q-NET: Grounded RL on Structured Language](https://arxiv.org/abs/1902.07257v1)\n",
    "\n",
    "But that's almost nothing in comparison to Atari games popularity. To fix this mistake, let's play with MiniWoB in this tutorial.\n",
    "\n",
    "This tutorial uses the original MiniWoB. There exists the imroved version from Stanford researches, called [MiniWoB++](https://stanfordnlp.github.io/miniwob-plusplus/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Why that's still relevant for RL\n",
    "\n",
    "You might ask, why do we need to play with two discontinued projects published two years ago (almost eternity by ML/DL field pace). There are several reasons for that:\n",
    "\n",
    "* MiniWoB problems are still very far from being solved. \n",
    "* Some of them are much harder and much close to real-life human behaviour than Atari games. \n",
    "* MiniWoB has practical application in automated software testing, web data extraction and automation.\n",
    "* Universe with all its issues, allows fast integration of applications into RL experiments without modifying the source code.\n",
    "* The most important: it's fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Architecture\n",
    "\n",
    "MiniWoB is implemented as a part of [OpenAI Universe](https://github.com/openai/universe) (another frozen project of OpenAI). The idea of Universe is to use VNC protocol to connect RL agent with GUI applications. As VNC is a cross-platform protocol used by humans to communicate with remote GUI applications, RL agent also achieves this ability (is RL agent smart enough to communicate is a different question). \n",
    "\n",
    "MiniWoB is a part of Universe, where GUI app is a browser with loaded dynamic webpages.\n",
    "\n",
    "Overall architecture of Universe is shown below\n",
    "\n",
    "![Arch](images/arch.png)\n",
    "\n",
    "The original MiniWoB docker image is available [on quay.io](https://quay.io/repository/openai/universe.world-of-bits), but I suggest you to use my version with fixed stability issues. The fixed version is available [on dockerhub](https://cloud.docker.com/u/shmuma/repository/docker/shmuma/miniwob). If you want to build your own version of the fixed image, you can follow [instructions here](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter13/wob_fixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Starting the container\n",
    "\n",
    "If you have docker installed, you can start the single container running the following command:\n",
    "\n",
    "`docker run -d -p 5900:5900 -p 15900:15900 --privileged --ipc host --cap-add SYS_ADMIN shmuma/miniwob run`\n",
    "\n",
    "Here, the options:\n",
    "* `-d` detaches container from terminal and keeps it running in background\n",
    "* `-p 5900:5900` forwards VNC port to the host machine\n",
    "* `-p 15900:15900` forwards rewarder port \n",
    "* `--privileged` gives extended privileges to this container (not sure is this needed or not, that was written in OpenAI manual)\n",
    "* `--ipc host` uses host's IPC namespace\n",
    "* `--cap-add SYS_ADMIN` extends container privileges\n",
    "* `shmuma/miniwob` name of the container image to start, you can use `quay.io/openai/universe.world-of- bits:0.20.0` to start original MiniWoB image\n",
    "* `run` command to start inside container\n",
    "\n",
    "During the training, several containers could be started (to decrease training samples correlation), to simplify this process, this repo includes two scripts: `containers_run.sh` and `containers_stop.sh`. The first starts the required amount of containers (given in command line), the second script stops all started containers (be careful, it just stops ALL containers, not only started by `containers_run.sh` script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\r\n",
      "IMAGE_NAME=shmuma/miniwob\r\n",
      "\r\n",
      "count=`docker ps -q | wc -l`\r\n",
      "\r\n",
      "if test $count -ne 0 ; then\r\n",
      "    echo You already have $count containers running, are you sure you want more?\r\n",
      "    exit\r\n",
      "fi\r\n",
      "\r\n",
      "for i in `seq 1 ${1:-1}`; do\r\n",
      "    echo Starting container $i\r\n",
      "    P1=$((5900+$i-1))\r\n",
      "    P2=$((15900+$i-1))\r\n",
      "    docker run -d -p $P1:5900 -p $P2:15900 --privileged --ipc host --cap-add SYS_ADMIN $IMAGE_NAME run\r\n",
      "done\r\n"
     ]
    }
   ],
   "source": [
    "!cat containers_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\r\n",
      "\r\n",
      "docker stop `docker ps -q`\r\n"
     ]
    }
   ],
   "source": [
    "!cat containers_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting container 1\n",
      "e94f531c4888a90d878b4383936eca99b4aa55cad8ce9e8e074a59b66396b531\n"
     ]
    }
   ],
   "source": [
    "!./containers_run.sh 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                  PORTS                                                        NAMES\r\n",
      "e94f531c4888        shmuma/miniwob      \"/app/universe-envs/â€¦\"   5 seconds ago       Up Less than a second   0.0.0.0:5900->5900/tcp, 5899/tcp, 0.0.0.0:15900->15900/tcp   xenodochial_varahamihira\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After this you can connect to the started container using one of many VNC clients available. If you're using MacOS, VNC client is already included in OS, in Finder press `Command+K` and then connect to 'vnc://localhost:5900'. Password for connection is `openai`\n",
    "\n",
    "![](images/vnc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Do not forget to stop containers, as they are quite CPU-hungry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af325a282347\r\n"
     ]
    }
   ],
   "source": [
    "!./containers_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/runtimes/__init__.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  spec = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import time\n",
    "import numpy as np\n",
    "import universe\n",
    "from typing import List, Optional, Tuple\n",
    "from universe import vectorized\n",
    "from universe.wrappers.experimental import SoftmaxClickMouse\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DOCKER_IMAGE = \"shmuma/miniwob:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# function to build connection endpoints for set of containers\n",
    "def remotes_url(port_ofs=0, hostname='localhost', count=8):\n",
    "    hosts = [\"%s:%d+%d\" % (hostname, 5900 + ofs, 15900 + ofs) for ofs in range(port_ofs, port_ofs+count)]\n",
    "    return \"vnc://\" + \",\".join(hosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym actions/observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universe vectorized observations/actions\n",
    "\n",
    "Before we start looking at actual MiniWoB action and observation spaces, we need to state one quite important difference between OpenAI Gym and OpenAI Universe. \n",
    "\n",
    "Gym API is **synchronous** in terms of agent's calls to `step()` function and internal environment state. For example, if the agent playing Atari decided to download the 10GB model file (between calls to `step()`), Atari 2600 emulator and the game it emulates notice nothing, as Atari knows nothing about wall clock time, only about ticks of 6507 MCU and internal emulated hardware state. Gym can do this, because it has the full control over every environment it exposes to the developer.\n",
    "\n",
    "In contrast, Universe has much less knowledge and control over the software it communicates with. VNC protocol provides remote framebuffer interface (low-level name of VNC protocol is RFB), capturing the image on the remote machine and transferring it to VNC client running in front of you. Your actions, like mouse movements and key presses are recorded and being sent to the VNC server. So, VNC is relatively thin protocol providing the impression that remote applications are running locally. This simplifies application integration, but at the same time if you've missed same frame from the input images stream, you've missed it. So, Universe Gym API is **asynchronous**:\n",
    "\n",
    "* agent needs to be fast enough to process observations and keep the desired framerate. This might be important in cases when something qick is happening on the screen\n",
    "* on `reset()` call, environment takes some time to start producing observations. To indicate this transient states, it returns None for observations, which means you need to wait and repeat the request.\n",
    "\n",
    "Another difference is in native support of several environments running in parallel. On creation of environment you can specify how many of them need to be created and system will start several docker images running in parallel. Alternatively, you can provide several ports of already running docker images (possibly on remote machines in the cloud). As a result, Universe `Env` instance returns vectors of observations from `reset()` and `step()` calls and expects you to provide vector of actions for `step()`. This breaks the API, but simplifies the development. \n",
    "\n",
    "Ptan library supports both vectorized and old-style environments, which also makes your life a little bit easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and actions\n",
    "\n",
    "Universe exposes two new subclasses of Gym `Space` class: [`VNCActionSpace`](https://github.com/openai/universe/blob/master/universe/spaces/vnc_action_space.py) and `VNCObservationSpace`\n",
    "\n",
    "Action space is a container for `VNCKeyEvent` or `VNCPointerEvent`: https://github.com/openai/universe/blob/master/universe/spaces/vnc_event.py\n",
    "\n",
    "Observation from single environment is a dictionary with two keys: `vision` with image pixels and `text` with text description from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have 1 containers running, are you sure you want more?\r\n"
     ]
    }
   ],
   "source": [
    "!./containers_run.sh 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:39:29,123] Making new env: wob.mini.ClickDialog-v0\n",
      "/home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "[2019-06-28 19:39:29,125] Writing logs to file: /tmp/universe-21835.log\n",
      "[2019-06-28 19:39:29,128] Using the golang VNC implementation\n",
      "[2019-06-28 19:39:29,129] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-06-28 19:39:29,206] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vnc://localhost:5900+15900\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wob.mini.ClickDialog-v0\")\n",
    "url = remotes_url(count=1)\n",
    "print(url)\n",
    "\n",
    "env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=5, vnc_kwargs={\n",
    "        'encoding': 'tight', 'compress_level': 0,\n",
    "        'fine_quality_level': 100, 'subsample_level': 0\n",
    "    })\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:39:33,279] Throttle fell behind by 3.87s; lost 19.35 frames\n",
      "[2019-06-28 19:39:45,636] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-06-28 19:39:46,144] [0:localhost:5900] Initial reset complete: episode_id=11\n"
     ]
    }
   ],
   "source": [
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got the first observation. The result is a list of single dict (remember vectorized form of universe observations?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vision': array([[[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]]], dtype=uint8), 'text': []}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1024x768 at 0x7F5374127DA0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.fromarray(obs[0]['vision'])\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image cropper\n",
    "\n",
    "As you can see, image is not cropped which is not very convenient, as relevant part of the problem's field is just tiny portion of the field. Below is the wrapper we'll use to crop the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniWoBCropper(vectorized.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Crops the WoB area and converts the observation into PyTorch (C, H, W) format.\n",
    "    \"\"\"\n",
    "    # Area of interest\n",
    "    WIDTH = 160\n",
    "    HEIGHT = 210\n",
    "    X_OFS = 10\n",
    "    Y_OFS = 75\n",
    "    \n",
    "    def __init__(self, env, keep_text=False):\n",
    "        super(MiniWoBCropper, self).__init__(env)\n",
    "        self.keep_text = keep_text\n",
    "        img_space = gym.spaces.Box(low=0, high=255, shape=(3, self.HEIGHT, self.WIDTH))\n",
    "        if keep_text:\n",
    "            self.observation_space = gym.spaces.Tuple(spaces=(img_space, gym.spaces.Space))\n",
    "        else:\n",
    "            self.observation_space = img_space\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        res = []\n",
    "        for obs in observation_n:\n",
    "            if obs is None:\n",
    "                res.append(obs)\n",
    "                continue\n",
    "            img = obs['vision'][self.Y_OFS:self.Y_OFS+self.HEIGHT, self.X_OFS:self.X_OFS+self.WIDTH, :]\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "            if self.keep_text:\n",
    "                text = \" \".join(map(lambda d: d.get('instruction', ''), obs.get('text', [{}])))\n",
    "                res.append((img, text))\n",
    "            else:\n",
    "                res.append(img)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny easter egg in MiniWoB: image area has exactly the same dimension as Atari games resultion in ALE.\n",
    "\n",
    "And please note that old gym doesn't support `dtype` in `Box` class, so, it will be wrongly set as float32 (but, in fact, byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:40:13,395] Making new env: wob.mini.ClickDialog-v0\n",
      "[2019-06-28 19:40:13,410] Using the golang VNC implementation\n",
      "[2019-06-28 19:40:13,410] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-06-28 19:40:13,433] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vnc://localhost:5900+15900\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wob.mini.ClickDialog-v0\")\n",
    "env = MiniWoBCropper(env)\n",
    "url = remotes_url(count=1)\n",
    "print(url)\n",
    "\n",
    "env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=5, vnc_kwargs={\n",
    "        'encoding': 'tight', 'compress_level': 0,\n",
    "        'fine_quality_level': 100, 'subsample_level': 0\n",
    "    })\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:40:15,990] Throttle fell behind by 2.36s; lost 11.78 frames\n",
      "[2019-06-28 19:40:29,771] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-06-28 19:40:29,797] [0:localhost:5900] Initial reset complete: episode_id=16\n"
     ]
    }
   ],
   "source": [
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[181, 182, 182, ..., 182, 182, 182],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [254, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]],\n",
       " \n",
       "        [[179, 180, 180, ..., 180, 180, 180],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]],\n",
       " \n",
       "        [[182, 183, 183, ..., 183, 183, 183],\n",
       "         [  0,   0,   0, ...,   0,   0,   0],\n",
       "         [  0,   0,   0, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]]], dtype=uint8)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 210, 160)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3, 210, 160)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x7F5374023550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.fromarray(obs[0].transpose((1, 2, 0)))\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "VNCActionSpace can include mouse click and key press events. To demonstrate how it is done, let's solve environment above by using brute force agent (sometimes this approach works better than sophisticated RL methods!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:41:04,629] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:41:12,920] Throttle fell behind by 8.09s; lost 40.45 frames\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got reward 0.768, clicks done 106\n",
      "Got reward 0.771, clicks done 14\n",
      "Got reward 0.597, clicks done 180\n",
      "Got reward 0.390, clicks done 33\n",
      "Got reward 0.971, clicks done 7\n",
      "Got reward 0.729, clicks done 118\n",
      "Got reward 0.951, clicks done 5\n",
      "Got reward 0.079, clicks done 101\n",
      "Got reward 0.079, clicks done 101\n",
      "Got reward 0.891, clicks done 8\n",
      "Mean reward for 10 solved episodes is 0.623\n"
     ]
    }
   ],
   "source": [
    "# try to solve 10 episodes by doing random clicking\n",
    "rewards = []\n",
    "clicks_done = 0\n",
    "\n",
    "while len(rewards) < 10:\n",
    "    clicks_done += 1\n",
    "    click_x = MiniWoBCropper.X_OFS + np.random.randint(MiniWoBCropper.WIDTH)\n",
    "    click_y = MiniWoBCropper.Y_OFS + np.random.randint(MiniWoBCropper.HEIGHT)\n",
    "\n",
    "    acts = [\n",
    "        universe.spaces.PointerEvent(click_x, click_y, buttonmask=0),\n",
    "        universe.spaces.PointerEvent(click_x, click_y, buttonmask=1),\n",
    "        universe.spaces.PointerEvent(click_x, click_y, buttonmask=0)\n",
    "    ]\n",
    "\n",
    "    obs_n, r_n, end_n, _ = env.step([acts])\n",
    "    if r_n[0] > 0:\n",
    "        rewards.append(r_n[0])\n",
    "        print(\"Got reward %.3f, clicks done %d\" % (r_n[0], clicks_done))\n",
    "        clicks_done = 0\n",
    "        \n",
    "print(\"Mean reward for %d solved episodes is %.3f\" % (len(rewards), np.mean(rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above approach are sampling from $160*210 = 33600$ action space, which might be too much for RL methods. To simplify our life a bit, we can discretize the area to be clicked by applying some grid and allowing to click only in the center of grid cell.\n",
    "\n",
    "<img src=\"images/wob-actions.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Universe already comes with such preprocessor, called `SoftmaxClickMouse`: https://github.com/openai/universe/blob/master/universe/wrappers/experimental/action_space.py#L65, let's try it in our random clicking approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-29 12:19:00,985] Making new env: wob.mini.ClickDialog-v0\n",
      "/home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "[2019-06-29 12:19:01,000] Using SoftmaxClickMouse with action_region=(10, 125, 170, 285), noclick_regions=[]\n",
      "[2019-06-29 12:19:01,000] SoftmaxClickMouse noclick regions removed 0 of 256 actions\n",
      "[2019-06-29 12:19:01,001] Writing logs to file: /tmp/universe-23640.log\n",
      "[2019-06-29 12:19:01,002] Using the golang VNC implementation\n",
      "[2019-06-29 12:19:01,002] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-06-29 12:19:01,016] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wob.mini.ClickDialog-v0\")\n",
    "env = MiniWoBCropper(env)\n",
    "env = SoftmaxClickMouse(env)\n",
    "url = remotes_url(count=1)\n",
    "\n",
    "env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=5, vnc_kwargs={\n",
    "        'encoding': 'tight', 'compress_level': 0,\n",
    "        'fine_quality_level': 100, 'subsample_level': 0\n",
    "    })\n",
    "\n",
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(256)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:43:41,958] Throttle fell behind by 7.79s; lost 38.97 frames\n",
      "[2019-06-28 19:43:50,299] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-06-28 19:43:55,371] [0:localhost:5900] Initial reset complete: episode_id=43\n"
     ]
    }
   ],
   "source": [
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:43:59,235] Throttle fell behind by 3.05s; lost 15.27 frames\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got reward 0.735, clicks done 16\n",
      "Got reward 0.971, clicks done 4\n",
      "Got reward 0.810, clicks done 12\n",
      "Got reward 0.391, clicks done 33\n",
      "Got reward 0.911, clicks done 7\n",
      "Got reward 0.149, clicks done 45\n",
      "Got reward 0.399, clicks done 85\n",
      "Got reward 0.771, clicks done 14\n",
      "Got reward 0.607, clicks done 232\n",
      "Got reward 0.250, clicks done 40\n",
      "Mean reward for 10 solved episodes is 0.599\n"
     ]
    }
   ],
   "source": [
    "# try to solve 10 episodes by doing random clicking\n",
    "rewards = []\n",
    "clicks_done = 0\n",
    "\n",
    "while len(rewards) < 10:\n",
    "    clicks_done += 1\n",
    "    act = env.action_space.sample()\n",
    "    obs_n, r_n, end_n, _ = env.step([act])\n",
    "    if r_n[0] > 0:\n",
    "        rewards.append(r_n[0])\n",
    "        print(\"Got reward %.3f, clicks done %d\" % (r_n[0], clicks_done))\n",
    "        clicks_done = 0\n",
    "print(\"Mean reward for %d solved episodes is %.3f\" % (len(rewards), np.mean(rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much improvements, probably RL will help with this :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DQN baseline\n",
    "\n",
    "With all those simplifications done, we can implement basic DQN agent to solve some environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-29 12:19:17,359] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "Below is the model we're going to use. Nothing fancy, just three convolution layers followed by two FC-layers, returning Q-values for our 256 click locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-29 12:19:19,384] [0:localhost:5900] Initial reset complete: episode_id=5807\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, 5, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batch preparation (practice)\n",
    "\n",
    "For data gathering we'll use [`ptan` library](https://github.com/shmuma/ptan/) which is a very thin RL-specific wrapper around Gym. It implement replay buffers, agent logic and other functions frequently required for RL training.\n",
    "\n",
    "Our first warm-up problem will be to write conversion of batch sampled from Replay Buffer into PyTorch tensors suitable for training.\n",
    "\n",
    "Input to the function is a list of `namedtuple` with the following fields ([source code](https://github.com/Shmuma/ptan/blob/master/ptan/experience.py#L155)):\n",
    "* `state`: state `s` in the trajectory, which is a 160x210 array of bytes with shape \\[3, 210, 160\\]\n",
    "* `action`: action executed, which is an integer from 0 to 255 indicating the square in our click grid\n",
    "* `reward`: immediate reward obtained after action execution\n",
    "* `last_state`: state we've got after execution of action. It equals to `None` if episode finished after action.\n",
    "\n",
    "We'll use 1-step DQN, but Ptan can do N-step calculation for you. You can experiment with the difference.\n",
    "\n",
    "The output should be three tensors containing the batch data with:\n",
    "* `state`: in a form of tensor with shape \\[X, Y, ...\\]\n",
    "* `actions`: as a long tensor\n",
    "* `ref values`: approximated Q-values using Bellman equation\n",
    "\n",
    "$Q_{s,a} = r_{s,a} + \\gamma \\cdot max_{a'}Q_{s',a'}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast], net: nn.Module, gamma: float, device=\"cpu\"):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []    # list of booleans, True if this is the last step in the episode\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        # unpack every experience entry into individual lists\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # convert everything to tensors (do not forget to put them into proper devices!)\n",
    "    # apply network and find best Q values of every action (torch.max() is the right function to use (c: )\n",
    "    # zero out q-values according to done_masks\n",
    "    # return the result as tuple of three things: states tensor, actions tensor and \n",
    "    # Q-values approximation tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tests for your solution\n",
    "\n",
    "There are several test cases you can use to check your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(5, 5))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2016, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code also should help you to understand the expectations from your code\n",
    "net = Model(env.observation_space.shape, env.action_space.n)\n",
    "# with eps=1, we don't need parent action selector, as all actions will be random, but that's a corner case!\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1) \n",
    "agent = ptan.agent.DQNAgent(net, action_selector)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 19:47:26,524] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, vectorized=True)\n",
    "batch = [e for _, e in zip(range(5), exp_source)]\n",
    "# add one final state with known reward\n",
    "batch.append(ptan.experience.ExperienceFirstLast(state=batch[0].state, action=0, reward=10.0, last_state=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = unpack_batch(batch, net, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert isinstance(r, tuple)\n",
    "assert len(r) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "states, actions, next_q = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert isinstance(states, torch.Tensor)\n",
    "assert states.dtype == torch.uint8\n",
    "assert states.size() == (6, 3, 210, 160)\n",
    "\n",
    "assert isinstance(actions, torch.Tensor)\n",
    "assert actions.dtype == torch.long\n",
    "assert actions.size() == (6, )\n",
    "\n",
    "assert isinstance(next_q, torch.Tensor)\n",
    "assert next_q.dtype == torch.float32\n",
    "assert next_q.size() == (6, )\n",
    "\n",
    "assert actions[5] == 0\n",
    "assert next_q[5] == 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batch preparation (solution)\n",
    "\n",
    "Please do not peek inside this, as you'll spoil all the fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast], net: nn.Module, gamma: float, device=\"cpu\"):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    last_states_v = torch.tensor(last_states).to(device)\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v + rewards_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Now we're ready to implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "REPLAY_SIZE = 10000\n",
    "MIN_REPLAY = 100\n",
    "TGT_SYNC = 100\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\"  \n",
    "#DEVICE = \"cpu\"\n",
    "\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.2\n",
    "STEPS_EPSILON = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 23:40:01,162] Making new env: wob.mini.ClickDialog-v0\n",
      "[2019-06-28 23:40:01,166] Using SoftmaxClickMouse with action_region=(10, 125, 170, 285), noclick_regions=[]\n",
      "[2019-06-28 23:40:01,167] SoftmaxClickMouse noclick regions removed 0 of 256 actions\n",
      "[2019-06-28 23:40:01,167] Using the golang VNC implementation\n",
      "[2019-06-28 23:40:01,168] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-06-28 23:40:01,191] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wob.mini.ClickDialog-v0\")\n",
    "env = MiniWoBCropper(env)\n",
    "env = SoftmaxClickMouse(env)\n",
    "url = remotes_url(count=1)\n",
    "\n",
    "env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=1, vnc_kwargs={\n",
    "        'encoding': 'tight', 'compress_level': 0,\n",
    "        'fine_quality_level': 100, 'subsample_level': 0\n",
    "    })\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 23:40:03,482] Throttle fell behind by 1.29s; lost 1.29 frames\n",
      "[2019-06-28 23:40:17,575] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-06-28 23:40:17,584] [0:localhost:5900] Initial reset complete: episode_id=1464\n"
     ]
    }
   ],
   "source": [
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code also should help you to understand the expectations from your code\n",
    "net = Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "parent_selector = ptan.actions.ArgmaxActionSelector()\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=INITIAL_EPSILON, selector=parent_selector) \n",
    "agent = ptan.agent.DQNAgent(net, action_selector, device=DEVICE)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, vectorized=True)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "optimizer = optim.Adam(net.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-28 23:40:19,641] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: Done 1 episodes, last 100 means: reward=-1.000, steps=8.000, speed=0.82 steps/s, eps=1.00\n",
      "20: Done 2 episodes, last 100 means: reward=-1.000, steps=9.500, speed=1.00 steps/s, eps=1.00\n",
      "23: Done 3 episodes, last 100 means: reward=-0.409, steps=7.333, speed=1.00 steps/s, eps=1.00\n",
      "33: Done 4 episodes, last 100 means: reward=-0.556, steps=8.000, speed=1.00 steps/s, eps=1.00\n",
      "37: Done 5 episodes, last 100 means: reward=-0.445, steps=7.200, speed=1.00 steps/s, eps=1.00\n",
      "47: Done 6 episodes, last 100 means: reward=-0.538, steps=7.667, speed=1.00 steps/s, eps=1.00\n",
      "58: Done 7 episodes, last 100 means: reward=-0.604, steps=8.143, speed=1.00 steps/s, eps=1.00\n",
      "68: Done 8 episodes, last 100 means: reward=-0.653, steps=8.375, speed=1.00 steps/s, eps=1.00\n",
      "79: Done 9 episodes, last 100 means: reward=-0.692, steps=8.667, speed=1.00 steps/s, eps=1.00\n",
      "89: Done 10 episodes, last 100 means: reward=-0.623, steps=8.800, speed=1.00 steps/s, eps=1.00\n",
      "100: Done 11 episodes, last 100 means: reward=-0.748, steps=9.000, speed=1.00 steps/s, eps=1.00\n",
      "100: nets synced, mean loss for last 100 steps = 0.144\n",
      "110: Done 12 episodes, last 100 means: reward=-0.769, steps=9.083, speed=1.00 steps/s, eps=0.89\n",
      "120: Done 13 episodes, last 100 means: reward=-0.787, steps=9.154, speed=1.00 steps/s, eps=0.88\n",
      "131: Done 14 episodes, last 100 means: reward=-0.802, steps=9.286, speed=1.00 steps/s, eps=0.87\n",
      "141: Done 15 episodes, last 100 means: reward=-0.815, steps=9.333, speed=1.00 steps/s, eps=0.86\n",
      "152: Done 16 episodes, last 100 means: reward=-0.827, steps=9.438, speed=1.00 steps/s, eps=0.85\n",
      "162: Done 17 episodes, last 100 means: reward=-0.837, steps=9.471, speed=1.00 steps/s, eps=0.84\n",
      "173: Done 18 episodes, last 100 means: reward=-0.846, steps=9.556, speed=1.00 steps/s, eps=0.83\n",
      "183: Done 19 episodes, last 100 means: reward=-0.801, steps=9.579, speed=1.00 steps/s, eps=0.82\n",
      "194: Done 20 episodes, last 100 means: reward=-0.811, steps=9.650, speed=1.00 steps/s, eps=0.81\n",
      "200: nets synced, mean loss for last 100 steps = 0.048\n",
      "204: Done 21 episodes, last 100 means: reward=-0.820, steps=9.667, speed=1.00 steps/s, eps=0.80\n",
      "215: Done 22 episodes, last 100 means: reward=-0.828, steps=9.727, speed=1.00 steps/s, eps=0.79\n",
      "225: Done 23 episodes, last 100 means: reward=-0.836, steps=9.739, speed=1.00 steps/s, eps=0.78\n",
      "236: Done 24 episodes, last 100 means: reward=-0.843, steps=9.792, speed=1.00 steps/s, eps=0.77\n",
      "246: Done 25 episodes, last 100 means: reward=-0.849, steps=9.800, speed=1.00 steps/s, eps=0.76\n",
      "249: Done 26 episodes, last 100 means: reward=-0.783, steps=9.538, speed=1.00 steps/s, eps=0.75\n",
      "259: Done 27 episodes, last 100 means: reward=-0.754, steps=9.556, speed=1.00 steps/s, eps=0.74\n",
      "269: Done 28 episodes, last 100 means: reward=-0.763, steps=9.571, speed=1.00 steps/s, eps=0.73\n",
      "280: Done 29 episodes, last 100 means: reward=-0.771, steps=9.621, speed=1.10 steps/s, eps=0.72\n",
      "281: Done 30 episodes, last 100 means: reward=-0.718, steps=9.367, speed=0.33 steps/s, eps=0.72\n",
      "291: Done 31 episodes, last 100 means: reward=-0.727, steps=9.387, speed=1.00 steps/s, eps=0.71\n",
      "300: nets synced, mean loss for last 100 steps = 0.072\n",
      "302: Done 32 episodes, last 100 means: reward=-0.735, steps=9.438, speed=1.00 steps/s, eps=0.70\n",
      "313: Done 33 episodes, last 100 means: reward=-0.743, steps=9.485, speed=1.00 steps/s, eps=0.69\n",
      "322: Done 34 episodes, last 100 means: reward=-0.717, steps=9.471, speed=1.00 steps/s, eps=0.68\n",
      "332: Done 35 episodes, last 100 means: reward=-0.725, steps=9.486, speed=1.00 steps/s, eps=0.67\n",
      "343: Done 36 episodes, last 100 means: reward=-0.733, steps=9.528, speed=1.00 steps/s, eps=0.66\n",
      "353: Done 37 episodes, last 100 means: reward=-0.740, steps=9.541, speed=1.00 steps/s, eps=0.65\n",
      "364: Done 38 episodes, last 100 means: reward=-0.747, steps=9.579, speed=1.00 steps/s, eps=0.64\n",
      "374: Done 39 episodes, last 100 means: reward=-0.753, steps=9.590, speed=1.00 steps/s, eps=0.63\n",
      "385: Done 40 episodes, last 100 means: reward=-0.760, steps=9.625, speed=1.00 steps/s, eps=0.62\n",
      "395: Done 41 episodes, last 100 means: reward=-0.765, steps=9.634, speed=1.00 steps/s, eps=0.61\n",
      "400: nets synced, mean loss for last 100 steps = 0.129\n",
      "406: Done 42 episodes, last 100 means: reward=-0.771, steps=9.667, speed=1.00 steps/s, eps=0.59\n",
      "416: Done 43 episodes, last 100 means: reward=-0.776, steps=9.674, speed=1.00 steps/s, eps=0.58\n",
      "427: Done 44 episodes, last 100 means: reward=-0.781, steps=9.705, speed=1.00 steps/s, eps=0.57\n",
      "437: Done 45 episodes, last 100 means: reward=-0.764, steps=9.711, speed=1.00 steps/s, eps=0.56\n",
      "448: Done 46 episodes, last 100 means: reward=-0.769, steps=9.739, speed=1.00 steps/s, eps=0.55\n",
      "458: Done 47 episodes, last 100 means: reward=-0.774, steps=9.745, speed=1.00 steps/s, eps=0.54\n",
      "469: Done 48 episodes, last 100 means: reward=-0.779, steps=9.771, speed=1.00 steps/s, eps=0.53\n",
      "479: Done 49 episodes, last 100 means: reward=-0.783, steps=9.776, speed=1.00 steps/s, eps=0.52\n",
      "490: Done 50 episodes, last 100 means: reward=-0.788, steps=9.800, speed=1.00 steps/s, eps=0.51\n",
      "500: Done 51 episodes, last 100 means: reward=-0.792, steps=9.804, speed=1.00 steps/s, eps=0.50\n",
      "500: nets synced, mean loss for last 100 steps = 0.186\n",
      "511: Done 52 episodes, last 100 means: reward=-0.796, steps=9.827, speed=1.00 steps/s, eps=0.49\n",
      "521: Done 53 episodes, last 100 means: reward=-0.800, steps=9.830, speed=1.00 steps/s, eps=0.48\n",
      "526: Done 54 episodes, last 100 means: reward=-0.773, steps=9.741, speed=1.00 steps/s, eps=0.47\n",
      "536: Done 55 episodes, last 100 means: reward=-0.778, steps=9.745, speed=1.00 steps/s, eps=0.46\n",
      "547: Done 56 episodes, last 100 means: reward=-0.782, steps=9.768, speed=1.00 steps/s, eps=0.45\n",
      "557: Done 57 episodes, last 100 means: reward=-0.785, steps=9.772, speed=1.00 steps/s, eps=0.44\n",
      "568: Done 58 episodes, last 100 means: reward=-0.789, steps=9.793, speed=1.00 steps/s, eps=0.43\n",
      "578: Done 59 episodes, last 100 means: reward=-0.776, steps=9.797, speed=1.00 steps/s, eps=0.42\n",
      "589: Done 60 episodes, last 100 means: reward=-0.796, steps=9.817, speed=1.00 steps/s, eps=0.41\n",
      "599: Done 61 episodes, last 100 means: reward=-0.799, steps=9.820, speed=1.00 steps/s, eps=0.40\n",
      "600: nets synced, mean loss for last 100 steps = 0.224\n",
      "610: Done 62 episodes, last 100 means: reward=-0.803, steps=9.839, speed=1.00 steps/s, eps=0.39\n",
      "620: Done 63 episodes, last 100 means: reward=-0.806, steps=9.841, speed=1.00 steps/s, eps=0.38\n",
      "623: Done 64 episodes, last 100 means: reward=-0.780, steps=9.734, speed=1.00 steps/s, eps=0.38\n",
      "633: Done 65 episodes, last 100 means: reward=-0.784, steps=9.738, speed=1.00 steps/s, eps=0.37\n",
      "643: Done 66 episodes, last 100 means: reward=-0.787, steps=9.742, speed=1.00 steps/s, eps=0.36\n",
      "653: Done 67 episodes, last 100 means: reward=-0.773, steps=9.746, speed=1.00 steps/s, eps=0.35\n",
      "663: Done 68 episodes, last 100 means: reward=-0.777, steps=9.750, speed=1.00 steps/s, eps=0.34\n",
      "674: Done 69 episodes, last 100 means: reward=-0.780, steps=9.768, speed=1.00 steps/s, eps=0.33\n",
      "682: Done 70 episodes, last 100 means: reward=-0.765, steps=9.743, speed=1.00 steps/s, eps=0.32\n",
      "692: Done 71 episodes, last 100 means: reward=-0.768, steps=9.746, speed=1.00 steps/s, eps=0.31\n",
      "700: nets synced, mean loss for last 100 steps = 0.278\n",
      "703: Done 72 episodes, last 100 means: reward=-0.772, steps=9.764, speed=1.00 steps/s, eps=0.30\n",
      "713: Done 73 episodes, last 100 means: reward=-0.775, steps=9.767, speed=1.00 steps/s, eps=0.29\n",
      "724: Done 74 episodes, last 100 means: reward=-0.778, steps=9.784, speed=1.00 steps/s, eps=0.28\n",
      "734: Done 75 episodes, last 100 means: reward=-0.781, steps=9.787, speed=1.00 steps/s, eps=0.27\n",
      "745: Done 76 episodes, last 100 means: reward=-0.784, steps=9.803, speed=1.00 steps/s, eps=0.26\n",
      "755: Done 77 episodes, last 100 means: reward=-0.786, steps=9.805, speed=1.00 steps/s, eps=0.25\n",
      "766: Done 78 episodes, last 100 means: reward=-0.789, steps=9.821, speed=1.00 steps/s, eps=0.23\n",
      "776: Done 79 episodes, last 100 means: reward=-0.792, steps=9.823, speed=1.00 steps/s, eps=0.22\n",
      "787: Done 80 episodes, last 100 means: reward=-0.794, steps=9.838, speed=1.00 steps/s, eps=0.21\n",
      "797: Done 81 episodes, last 100 means: reward=-0.797, steps=9.840, speed=1.00 steps/s, eps=0.20\n",
      "800: Done 82 episodes, last 100 means: reward=-0.777, steps=9.756, speed=1.00 steps/s, eps=0.20\n",
      "800: nets synced, mean loss for last 100 steps = 0.293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810: Done 83 episodes, last 100 means: reward=-0.780, steps=9.759, speed=1.00 steps/s, eps=0.20\n",
      "820: Done 84 episodes, last 100 means: reward=-0.783, steps=9.762, speed=1.00 steps/s, eps=0.20\n",
      "831: Done 85 episodes, last 100 means: reward=-0.785, steps=9.776, speed=1.00 steps/s, eps=0.20\n",
      "841: Done 86 episodes, last 100 means: reward=-0.788, steps=9.779, speed=1.00 steps/s, eps=0.20\n",
      "852: Done 87 episodes, last 100 means: reward=-0.790, steps=9.793, speed=1.00 steps/s, eps=0.20\n",
      "862: Done 88 episodes, last 100 means: reward=-0.792, steps=9.795, speed=1.00 steps/s, eps=0.20\n",
      "873: Done 89 episodes, last 100 means: reward=-0.795, steps=9.809, speed=1.00 steps/s, eps=0.20\n",
      "883: Done 90 episodes, last 100 means: reward=-0.797, steps=9.811, speed=1.00 steps/s, eps=0.20\n",
      "894: Done 91 episodes, last 100 means: reward=-0.799, steps=9.824, speed=1.00 steps/s, eps=0.20\n",
      "900: nets synced, mean loss for last 100 steps = 0.302\n",
      "904: Done 92 episodes, last 100 means: reward=-0.801, steps=9.826, speed=1.00 steps/s, eps=0.20\n",
      "915: Done 93 episodes, last 100 means: reward=-0.804, steps=9.839, speed=1.00 steps/s, eps=0.20\n",
      "925: Done 94 episodes, last 100 means: reward=-0.806, steps=9.840, speed=1.00 steps/s, eps=0.20\n",
      "936: Done 95 episodes, last 100 means: reward=-0.808, steps=9.853, speed=1.00 steps/s, eps=0.20\n",
      "939: Done 96 episodes, last 100 means: reward=-0.791, steps=9.781, speed=1.00 steps/s, eps=0.20\n",
      "949: Done 97 episodes, last 100 means: reward=-0.793, steps=9.784, speed=1.00 steps/s, eps=0.20\n",
      "959: Done 98 episodes, last 100 means: reward=-0.795, steps=9.786, speed=1.00 steps/s, eps=0.20\n",
      "970: Done 99 episodes, last 100 means: reward=-0.797, steps=9.798, speed=1.00 steps/s, eps=0.20\n",
      "980: Done 100 episodes, last 100 means: reward=-0.799, steps=9.800, speed=1.00 steps/s, eps=0.20\n",
      "991: Done 101 episodes, last 100 means: reward=-0.799, steps=9.830, speed=1.00 steps/s, eps=0.20\n",
      "1000: nets synced, mean loss for last 100 steps = 0.436\n",
      "1001: Done 102 episodes, last 100 means: reward=-0.788, steps=9.820, speed=1.00 steps/s, eps=0.20\n",
      "1011: Done 103 episodes, last 100 means: reward=-0.806, steps=9.890, speed=1.00 steps/s, eps=0.20\n",
      "1022: Done 104 episodes, last 100 means: reward=-0.806, steps=9.900, speed=1.00 steps/s, eps=0.20\n",
      "1032: Done 105 episodes, last 100 means: reward=-0.806, steps=9.960, speed=1.00 steps/s, eps=0.20\n",
      "1043: Done 106 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1053: Done 107 episodes, last 100 means: reward=-0.806, steps=9.960, speed=1.00 steps/s, eps=0.20\n",
      "1064: Done 108 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1074: Done 109 episodes, last 100 means: reward=-0.806, steps=9.960, speed=1.00 steps/s, eps=0.20\n",
      "1085: Done 110 episodes, last 100 means: reward=-0.816, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1095: Done 111 episodes, last 100 means: reward=-0.806, steps=9.960, speed=1.00 steps/s, eps=0.20\n",
      "1100: nets synced, mean loss for last 100 steps = 0.444\n",
      "1106: Done 112 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1116: Done 113 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1127: Done 114 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1137: Done 115 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1148: Done 116 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1158: Done 117 episodes, last 100 means: reward=-0.796, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1169: Done 118 episodes, last 100 means: reward=-0.806, steps=9.970, speed=1.00 steps/s, eps=0.20\n",
      "1174: Done 119 episodes, last 100 means: reward=-0.801, steps=9.920, speed=1.00 steps/s, eps=0.20\n",
      "1184: Done 120 episodes, last 100 means: reward=-0.801, steps=9.910, speed=1.00 steps/s, eps=0.20\n",
      "1195: Done 121 episodes, last 100 means: reward=-0.801, steps=9.920, speed=1.00 steps/s, eps=0.20\n",
      "1200: nets synced, mean loss for last 100 steps = 0.595\n",
      "1204: Done 122 episodes, last 100 means: reward=-0.789, steps=9.900, speed=1.00 steps/s, eps=0.20\n",
      "1214: Done 123 episodes, last 100 means: reward=-0.789, steps=9.900, speed=1.00 steps/s, eps=0.20\n",
      "1225: Done 124 episodes, last 100 means: reward=-0.789, steps=9.900, speed=1.00 steps/s, eps=0.20\n",
      "1235: Done 125 episodes, last 100 means: reward=-0.789, steps=9.900, speed=1.00 steps/s, eps=0.20\n",
      "1246: Done 126 episodes, last 100 means: reward=-0.798, steps=9.980, speed=1.00 steps/s, eps=0.20\n",
      "1256: Done 127 episodes, last 100 means: reward=-0.818, steps=9.980, speed=1.00 steps/s, eps=0.20\n",
      "1267: Done 128 episodes, last 100 means: reward=-0.818, steps=9.990, speed=1.00 steps/s, eps=0.20\n",
      "1277: Done 129 episodes, last 100 means: reward=-0.818, steps=9.980, speed=1.00 steps/s, eps=0.20\n",
      "1288: Done 130 episodes, last 100 means: reward=-0.836, steps=10.070, speed=1.00 steps/s, eps=0.20\n",
      "1298: Done 131 episodes, last 100 means: reward=-0.836, steps=10.070, speed=1.00 steps/s, eps=0.20\n",
      "1300: nets synced, mean loss for last 100 steps = 0.676\n",
      "1309: Done 132 episodes, last 100 means: reward=-0.836, steps=10.070, speed=1.00 steps/s, eps=0.20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-aa7bc7c1a6b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_rewards_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrouped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mnext_state_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/wrappers/timer.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpyprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnc_env.Timer.step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Calculate how much time was spent actually doing work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/wrappers/render.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/miniwob/lib/python3.6/site-packages/universe/wrappers/throttle.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mpyprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnc_env.Throttle.sleep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0maccum_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stats.throttle.sleep'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;31m# We want to merge in the latest reward/done/info so that our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "losses = []\n",
    "steps = 0\n",
    "last_ts = time.time()\n",
    "last_steps = 0\n",
    "\n",
    "while True:\n",
    "    steps += 1\n",
    "    buffer.populate(1)\n",
    "    r = exp_source.pop_rewards_steps()\n",
    "    if r:\n",
    "        for rw, st in r:\n",
    "            episode_rewards.append(rw)\n",
    "            episode_steps.append(st)\n",
    "        speed = (steps - last_steps) / (time.time() - last_ts)\n",
    "        print(\"%d: Done %d episodes, last 100 means: reward=%.3f, steps=%.3f, speed=%.2f steps/s, eps=%.2f\" % (\n",
    "            steps, len(episode_rewards), np.mean(episode_rewards[-100:]), \n",
    "            np.mean(episode_steps[-100:]), speed, action_selector.epsilon\n",
    "        ))\n",
    "        last_ts = time.time()\n",
    "        last_steps = steps\n",
    "        if np.mean(episode_rewards[-100:]) > 0.9:\n",
    "            print(\"You solved the env! Congrats!\")\n",
    "            break\n",
    "    if len(buffer) < MIN_REPLAY:\n",
    "        continue\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    state_v, actions_v, ref_q_v = unpack_batch(batch, tgt_net.target_model, gamma=GAMMA, device=DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    q_v = net(state_v)\n",
    "    q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    loss_v = F.mse_loss(q_v, ref_q_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss_v.item())\n",
    "    \n",
    "    if steps % TGT_SYNC == 0:\n",
    "        print(\"%d: nets synced, mean loss for last 100 steps = %.3f\" % (\n",
    "            steps, np.mean(losses[-100:])))\n",
    "        tgt_net.sync()\n",
    "    action_selector.epsilon = max(FINAL_EPSILON, INITIAL_EPSILON - steps / STEPS_EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting of Baseline DQN\n",
    "\n",
    "> \"You love Linux, probably you're experienced in troubleshooting...\"\n",
    "> RHCE Exam Instructor @ 2005\n",
    "\n",
    "Code above doesn't converge. Usually, in such situations, tutorial author tweaks the parameters to make it working, so, attendants see only final, polished version. This makes the false impression of ML as smooth and quite obvious process, which very far from truth. Don't know about others, but my code doesn't work 90% of the time :).\n",
    "\n",
    "To learn how to deal with such situations, you're asked to troubleshoot the code above. There are several directions you could explore (of course, you can have your own ideas, that's not the complete list):\n",
    "\n",
    "* check training samples. Do they make sense? Is reward properly assigned to correct observations? Rememer, MiniWoB could have bugs\n",
    "* training gradients. How large they are? What's the ratio of gradients/weight values?\n",
    "* do end-of-episode steps handled properly? Final steps of epsiodes act as an anchor to prevent Q values from growing infinitely, so, it is critical to have them properly handled in Bellman equation\n",
    "* explore Q values produced by the network during the training. Are they growing over time? That's a good practice to have small set of states (including final step and steps before the final) and track their Qs during the training.\n",
    "* how large the difference between trained network and target net used for next-step Bellman approximation? How it changes over time?\n",
    "\n",
    "Full check might take a day, especially if you haven't done it before. In separate notebook (01_miniwob_baseline_debug) you will be shown how to do those checks in this environment, but you should try implement them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training samples check\n",
    "\n",
    "Here we're going to eyeball our training samples. OpenAI Universe is asynchronous, so, if we're missing a frame or have delays to handle input data, we can get weird training samples.\n",
    "\n",
    "To do this, we implement small `Env` wrapper applied after `MiniWoBCropper` to keep history of training sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryTracking(vectorized.Wrapper):\n",
    "    \"\"\"\n",
    "    Keeps last N trajectories from the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, env, size: int):\n",
    "        super(TrajectoryTracking, self).__init__(env)\n",
    "        self.trajectories = collections.deque(maxlen=size)\n",
    "        self._in_progress = []\n",
    "\n",
    "    def reset(self):\n",
    "        obs_n = self.env.reset()\n",
    "        if not self._in_progress:\n",
    "            self._in_progress = [[] for _ in obs_n]\n",
    "        for t, obs in zip(self._in_progress, obs_n):\n",
    "            if t:\n",
    "                self.trajectories.append(list(t))\n",
    "                t.clear()\n",
    "            t.append(obs)\n",
    "        return obs_n\n",
    "    \n",
    "    def step(self, action_n):\n",
    "        obs_n, r_n, done_n, info_n = self.env.step(action_n)\n",
    "        for t, obs, r, act in zip(self._in_progress, obs_n, r_n, action_n):\n",
    "            t.append((obs, act, r))\n",
    "        for t, done in zip(self._in_progress, done_n):\n",
    "            if done:\n",
    "                self.trajectories.append(list(t))\n",
    "                t.clear()\n",
    "        return obs_n, r_n, done_n, info_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to our training to get trajectories to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-29 13:06:28,484] Making new env: wob.mini.ClickDialog-v0\n",
      "/home/shmuma/anaconda3/envs/miniwob/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "[2019-06-29 13:06:28,487] Using SoftmaxClickMouse with action_region=(10, 125, 170, 285), noclick_regions=[]\n",
      "[2019-06-29 13:06:28,488] SoftmaxClickMouse noclick regions removed 0 of 256 actions\n",
      "[2019-06-29 13:06:28,488] Using the golang VNC implementation\n",
      "[2019-06-29 13:06:28,489] Using VNCSession arguments: {'encoding': 'tight', 'compress_level': 0, 'fine_quality_level': 100, 'subsample_level': 0, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2019-06-29 13:06:28,506] [0] Connecting to environment: vnc://localhost:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15900/viewer/?password=openai\n",
      "[2019-06-29 13:06:44,843] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n",
      "[2019-06-29 13:06:44,889] [0:localhost:5900] Initial reset complete: episode_id=6078\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"wob.mini.ClickDialog-v0\")\n",
    "env = MiniWoBCropper(env)\n",
    "env = SoftmaxClickMouse(env)\n",
    "env = TrajectoryTracking(env, size=10)\n",
    "url = remotes_url(count=1)\n",
    "\n",
    "env.configure(remotes=url, docker_image=DOCKER_IMAGE, fps=1, vnc_kwargs={\n",
    "        'encoding': 'tight', 'compress_level': 0,\n",
    "        'fine_quality_level': 100, 'subsample_level': 0\n",
    "    })\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-29 13:06:46,438] Throttle fell behind by 16.93s; lost 16.93 frames\n"
     ]
    }
   ],
   "source": [
    "while obs[0] is None:\n",
    "    a = env.action_space.sample()\n",
    "    obs, reward, is_done, info = env.step([a])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "\n",
    "parent_selector = ptan.actions.ArgmaxActionSelector()\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1, selector=parent_selector) \n",
    "agent = ptan.agent.DQNAgent(net, action_selector, device=DEVICE)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, vectorized=True)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-29 13:10:05,696] [0:localhost:5900] Sending reset for env_id=wob.mini.ClickDialog-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done with 12 steps and -1.000 reward\n",
      "Episode done with 5 steps and 0.643 reward\n",
      "Episode done with 8 steps and 0.000 reward\n",
      "Episode done with 8 steps and 0.501 reward\n",
      "Episode done with 10 steps and -1.000 reward\n",
      "Episode done with 11 steps and -1.000 reward\n",
      "Episode done with 10 steps and -1.000 reward\n",
      "Episode done with 11 steps and -1.000 reward\n",
      "Episode done with 10 steps and -1.000 reward\n",
      "Episode done with 7 steps and 0.403 reward\n"
     ]
    }
   ],
   "source": [
    "# populate buffer to get some trajectories\n",
    "for _ in range(100):\n",
    "    buffer.populate(1)\n",
    "    for rw, st in exp_source.pop_rewards_steps():\n",
    "        print(\"Episode done with %d steps and %.3f reward\" % (st, rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 8, 8, 10, 11, 10, 11, 10, 7, 10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, env.trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to select different trajectory to check, I've chosen not very long one with positive reward\n",
    "tr = env.trajectories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_obs, tr_act, tr_rw = zip(*tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 4, 126, 133, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0, 0.0, 0.6432)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_img(obs, action=None, action_step_pix=10, action_y_ofs=50, transpose=True):\n",
    "    \"\"\"\n",
    "    Get the image from observation with optional action place shown\n",
    "    \"\"\"\n",
    "    if transpose:\n",
    "        obs = np.transpose(obs, (1, 2, 0))\n",
    "    img = Image.fromarray(obs)\n",
    "    if action is not None:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        if isinstance(action, tuple):\n",
    "            x_ofs, y_ofs = action\n",
    "            x_ofs -= X_OFS\n",
    "            y_ofs -= Y_OFS\n",
    "        else:\n",
    "            y_ofs = action_y_ofs + (action % 16) * action_step_pix\n",
    "            x_ofs = (action // 16) * action_step_pix\n",
    "        half_step = action_step_pix//2\n",
    "        draw.ellipse((x_ofs-half_step, y_ofs-half_step, x_ofs+half_step, y_ofs+half_step),\n",
    "                     (0, 0, 255, 128))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [get_obs_img(obs, action=act) for obs, act in zip(tr_obs, tr_act)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, img in enumerate(imgs, start=1):\n",
    "    fig.add_subplot(1, len(imgs), i)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, that doesn't look OK. We've clicked on the wrong places and still got positive reward, let's check another trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not forget to stop container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0402f763515b\r\n"
     ]
    }
   ],
   "source": [
    "!./containers_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
